%==============================================================================
% SECTION 1: METHODS OVERVIEW
%==============================================================================

\subsection{Architectural Overview}
\label{sec:overview}

MofNeuroSim implements IEEE 754 floating-point arithmetic entirely through spiking neural networks (SNNs) using Generalized Leaky Integrate-and-Fire (GLIF) neurons. As established in Section~\ref{sec:theory}, GLIF networks with soft reset and dynamic thresholds provide the theoretical foundations for bit-exact computation. All computations are performed through spike-based logic gates constructed from GLIF neurons.

\subsubsection{Computation Flow}

The general computation flow follows a three-stage pipeline as given by Equation~\ref{eq:flow}:

\begin{equation}
\text{Float} \xrightarrow{\text{Encoder}} \text{Pulse} \xrightarrow{\text{SNN Gates}} \text{Pulse} \xrightarrow{\text{Decoder}} \text{Float}
\label{eq:flow}
\end{equation}
where $\text{Float}$ denotes the floating-point domain, $\text{Pulse}$ denotes the binary pulse domain, $\text{Encoder}$ denotes the boundary operation converting floats to pulses, $\text{SNN Gates}$ denotes the spike-based computation, and $\text{Decoder}$ denotes the boundary operation converting pulses back to floats. The encoder and decoder serve as \textbf{boundary operations} between the continuous floating-point domain and the discrete pulse domain. Within the pulse domain, all operations are performed using SNN gates that process binary spike sequences.

\subsubsection{Hierarchical Architecture}

The system is organized into five hierarchical levels, each building upon the previous:

\begin{itemize}
    \item \textbf{Level 0 -- Neurons}: Generalized Leaky Integrate-and-Fire (GLIF) neurons providing the computational primitives. With decay factor $\beta = 1$, GLIF reduces to ideal IF neurons for bit-exact digital logic; with $\beta < 1$, it becomes LIF for physical hardware simulation. Both variants use soft reset to preserve the toroidal phase space topology (Theorem~\ref{thm:glif_embedding}).

    \item \textbf{Level 1 -- Logic Gates}: Boolean gates (AND, OR, NOT, XOR, MUX) constructed from GLIF neurons with fixed thresholds. These gates process binary spike values.

    \item \textbf{Level 2 -- Arithmetic Units}: Multi-bit adders, subtractors, comparators, and shifters built from Level 1 gates. These implement integer arithmetic on pulse sequences.

    \item \textbf{Level 3 -- Floating-Point Operators}: IEEE 754 compliant operators (adder, multiplier, divider, square root) that process sign, exponent, and mantissa fields separately using Level 2 units.

    \item \textbf{Level 4 -- Neural Network Layers}: Complete layers (Linear, LayerNorm, RMSNorm, Attention) and activation functions (Sigmoid, Tanh, GELU, Softmax) built from Level 3 operators.
\end{itemize}

\subsubsection{Notation and Symbols}

Table~\ref{tab:notation} summarizes the notation used throughout this paper.

\begin{table}[t]
\centering
\caption{Notation and Symbols}
\label{tab:notation}
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Symbol} & \textbf{Description} \\
\midrule
$\pulse{x}$ & Pulse representation of value $x$ \\
$\spike$ & Single spike (binary: 0 or 1) \\
$\membrane$ & Membrane potential \\
$\threshold$ & Firing threshold \\
$\heaviside(\cdot)$ & Heaviside step function \\
$s, e, m$ & Sign, exponent, mantissa bits \\
$n_e, n_m$ & Number of exponent/mantissa bits \\
$\text{bias}$ & Exponent bias ($2^{n_e-1} - 1$) \\
$\beta$ & LIF decay factor ($0 < \beta < 1$) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Supported Precisions}

MofNeuroSim supports multiple IEEE 754 floating-point formats, each achieving bit-exact results:

\begin{table}[t]
\centering
\caption{Supported Floating-Point Formats}
\label{tab:formats}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Format} & \textbf{Bits} & \textbf{Sign} & \textbf{Exp} & \textbf{Mant} \\
\midrule
FP8 (E4M3) & 8 & 1 & 4 & 3 \\
FP16 & 16 & 1 & 5 & 10 \\
FP32 & 32 & 1 & 8 & 23 \\
FP64 & 64 & 1 & 11 & 52 \\
\bottomrule
\end{tabular}
\end{table}

