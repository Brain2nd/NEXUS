%==============================================================================
% SECTION 5: ACTIVATION FUNCTIONS
%==============================================================================

\subsection{Activation Functions}
\label{sec:activation}

This section describes how common neural network activation functions are implemented using SNN arithmetic operators. Each function is decomposed into floating-point operations from Section~\ref{sec:arithmetic}.

\subsubsection{Exponential Function}
\label{sec:exp}

The exponential function $e^x$ is fundamental to many activations. We implement it using a table-driven algorithm entirely through SNN operators.

\paragraph{Algorithm Overview}
The computation uses $N=32$ subdivisions per octave as given by Equation~\ref{eq:exp_decomp}:

\begin{equation}
z = \text{round}(x \cdot N / \ln 2), \quad k = \lfloor z/N \rfloor, \quad j = z \mod N
\label{eq:exp_decomp}
\end{equation}
where $z$ denotes the scaled input, $k$ denotes the integer exponent, and $j \in [0, 31]$ denotes the table index.

\paragraph{SNN Implementation}
The computation proceeds entirely through SNN gates:
\begin{itemize}
    \item \textbf{Scaling}: $x \cdot (N/\ln 2)$ via SNN multiplier with precomputed constant
    \item \textbf{Rounding}: SNN floor function using comparators and MUX gates
    \item \textbf{Table lookup}: 5-layer MUX tree selects from 32 precomputed $T[j] = 2^{j/32}$ constants stored as pulse sequences
    \item \textbf{Remainder}: $r = x - z \cdot \ln 2$ via SNN subtraction
\end{itemize}

\paragraph{Polynomial Evaluation}
A degree-3 minimax polynomial approximates $e^r$ for $|r| < \ln 2/64$:

\begin{equation}
P(r) = 1 + r + C_2 r^2 + C_3 r^3
\label{eq:exp_poly}
\end{equation}
where $C_2, C_3$ are precomputed pulse constants. Horner's method ($P = 1 + r(1 + r(C_2 + r \cdot C_3))$) minimizes SNN multiplications.

\paragraph{Result Reconstruction}
The final result combines components using SNN operators:

\begin{equation}
e^x = 2^k \times T[j] \times P(r)
\label{eq:exp_result}
\end{equation}
where $2^k$ scaling adjusts the exponent field directly via SNN bit manipulation, and $T[j] \times P(r)$ uses SNN multiplication.

\subsubsection{Sigmoid Function}
\label{sec:sigmoid}

The sigmoid activation is defined by Equation~\ref{eq:sigmoid}:

\begin{equation}
\sigma(x) = \frac{1}{1 + e^{-x}}
\label{eq:sigmoid}
\end{equation}
where $x$ denotes the input value and $\sigma(x) \in (0, 1)$ denotes the output.

\paragraph{SNN Implementation}
The computation proceeds as: negate ($x \to -x$), exponential ($e^{-x}$), add one ($1+e^{-x}$), then reciprocal via Newton-Raphson to obtain $1/(1+e^{-x})$.

\paragraph{Symmetry Property}
For numerical stability with large positive inputs:

\begin{equation}
\sigma(x) = 1 - \sigma(-x)
\end{equation}

When $x > 0$, we compute $\sigma(-x)$ to avoid overflow in $e^{-x}$.

\subsubsection{Hyperbolic Tangent}
\label{sec:tanh}

The tanh activation is related to sigmoid as given by Equation~\ref{eq:tanh}:

\begin{equation}
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{e^{2x} - 1}{e^{2x} + 1} = 2\sigma(2x) - 1
\label{eq:tanh}
\end{equation}
where $x$ denotes the input value, $\sigma(\cdot)$ denotes the sigmoid function, and $\tanh(x) \in (-1, 1)$ denotes the output.

\paragraph{SNN Implementation}
Using the sigmoid-based form: scale ($x \to 2x$), apply sigmoid ($\sigma(2x)$), scale again ($2\sigma(2x)$), then subtract one to obtain $2\sigma(2x) - 1$.

\subsubsection{GELU (Gaussian Error Linear Unit)}
\label{sec:gelu}

GELU is defined by Equation~\ref{eq:gelu_exact}:

\begin{equation}
\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
\label{eq:gelu_exact}
\end{equation}
where $x$ denotes the input value, $\Phi(x)$ denotes the Gaussian CDF, and $\text{erf}(\cdot)$ denotes the error function.

\paragraph{Fast Approximation}
We use the sigmoid-based approximation given by Equation~\ref{eq:gelu_approx}:

\begin{equation}
\text{GELU}(x) \approx x \cdot \sigma(1.702 \, x)
\label{eq:gelu_approx}
\end{equation}
where $\sigma(\cdot)$ denotes the sigmoid function and 1.702 is an empirically determined constant. This approximation has maximum error $< 0.005$.

\paragraph{SNN Implementation}
The computation proceeds as: scale ($x \to 1.702x$), apply sigmoid ($\sigma(1.702x)$), then multiply by $x$ to obtain $x \cdot \sigma(1.702x)$.

\paragraph{Exact GELU}
For applications requiring higher precision, we implement the exact formula using the error function Taylor series given by Equation~\ref{eq:erf_taylor}:

\begin{equation}
\text{erf}(x) = \frac{2}{\sqrt{\pi}} \sum_{n=0}^{N} \frac{(-1)^n x^{2n+1}}{n!(2n+1)}
\label{eq:erf_taylor}
\end{equation}
where $\text{erf}(x)$ denotes the error function, $x$ denotes the input value, and $N$ denotes the number of Taylor series terms.

\subsubsection{SiLU (Sigmoid Linear Unit)}
\label{sec:silu}

SiLU, also known as Swish, is defined by Equation~\ref{eq:silu}:

\begin{equation}
\text{SiLU}(x) = x \cdot \sigma(x)
\label{eq:silu}
\end{equation}
where $x$ denotes the input value and $\sigma(\cdot)$ denotes the sigmoid function.

\paragraph{SNN Implementation}
The computation proceeds as: apply sigmoid ($\sigma(x)$), then multiply by $x$ to obtain $x \cdot \sigma(x)$.

\subsubsection{ReLU (Rectified Linear Unit)}
\label{sec:relu}

ReLU is the simplest activation as defined by Equation~\ref{eq:relu}:

\begin{equation}
\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x \geq 0 \\ 0 & \text{if } x < 0 \end{cases}
\label{eq:relu}
\end{equation}
where $x$ denotes the input value.

\paragraph{SNN Implementation}
ReLU is implemented by checking the sign bit as given by Equation~\ref{eq:relu_snn}:

\begin{equation}
\text{ReLU}(\pulse{x}) = \text{AND}(\pulse{x}, \text{NOT}(s_x))
\label{eq:relu_snn}
\end{equation}
where $\pulse{x}$ denotes the pulse representation of $x$ and $s_x$ denotes the sign bit (pulse 0 of $\pulse{x}$). When $s_x = 1$ (negative), all bits are zeroed.

This requires only a single AND gate per bit plus one NOT gate for the sign, making it extremely efficient.

\subsubsection{Softmax}
\label{sec:softmax}

Softmax converts a vector of logits to probabilities as given by Equation~\ref{eq:softmax}:

\begin{equation}
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\label{eq:softmax}
\end{equation}
where $x_i$ denotes the $i$-th input logit, $n$ denotes the number of elements, and the output represents a probability distribution that sums to 1.

\paragraph{Numerical Stability}
For stability, we subtract the maximum value as given by Equation~\ref{eq:softmax_stable}:

\begin{equation}
\text{Softmax}(x_i) = \frac{e^{x_i - x_{\max}}}{\sum_{j=1}^{n} e^{x_j - x_{\max}}}
\label{eq:softmax_stable}
\end{equation}
where $x_{\max} = \max_j(x_j)$ denotes the maximum input value. This prevents overflow in the exponential computation.

\paragraph{SNN Implementation}
The computation proceeds as: find maximum ($x_{\max}$) using SNN comparators, subtract ($x'_i = x_i - x_{\max}$), compute exponentials ($e_i = e^{x'_i}$), sum ($S = \sum_j e_j$) using an adder tree, then normalize ($p_i = e_i / S$) using SNN division.

\subsubsection{Complexity Analysis}

Table~\ref{tab:activation_complexity} summarizes the computational complexity of each activation:

\begin{table}[t]
\centering
\caption{Activation Function Complexity (FP32)}
\label{tab:activation_complexity}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Activation} & \textbf{Operations} & \textbf{Cost} \\
\midrule
ReLU & 1 NOT, 32 AND & 1$\times$ \\
Sigmoid & exp, div & 15$\times$ \\
Tanh & sigmoid, mul, sub & 18$\times$ \\
GELU (fast) & sigmoid, 2 mul & 20$\times$ \\
SiLU & sigmoid, mul & 17$\times$ \\
Softmax & exp, div & 15$\times$ \\
\bottomrule
\end{tabular}
\end{table}

