\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bachinin et~al.(2024)]{bachinin2024mof}
Bachinin, S.~V. et~al.
\newblock Metal-organic framework single crystal for in-memory neuromorphic
  computing.
\newblock \emph{Nature Communications}, 2024.

\bibitem[Chen et~al.(2024)Chen, She, Wang, and Duan]{chen2024memristive}
Chen, T., She, C., Wang, L., and Duan, S.
\newblock Memristive leaky integrate-and-fire neuron and learnable
  straight-through estimator in spiking neural networks.
\newblock \emph{Cognitive Neurodynamics}, 2024.

\bibitem[Date et~al.(2023)]{date2023encoding}
Date, P. et~al.
\newblock Encoding integers and rationals on neuromorphic computers using
  virtual neuron.
\newblock \emph{Scientific Reports}, 13\penalty0 (1):\penalty0 8987, 2023.

\bibitem[Davies et~al.(2021)]{davies2021advancing}
Davies, M. et~al.
\newblock Advancing neuromorphic computing with loihi: A survey of results and
  outlook.
\newblock \emph{Proceedings of the IEEE}, 109\penalty0 (5):\penalty0 911--934,
  2021.

\bibitem[Dubey et~al.(2020)Dubey, Raj, et~al.]{dubey2020floating}
Dubey, K., Raj, B., et~al.
\newblock Floating-point multiplication using neuromorphic computing.
\newblock \emph{arXiv preprint arXiv:2008.13245}, 2020.

\bibitem[George et~al.(2019{\natexlab{a}})George, Banerjee, and
  Suri]{george2019ieee}
George, A.~M., Banerjee, D., and Suri, M.
\newblock Ieee 754 floating-point addition for neuromorphic architecture.
\newblock \emph{Neurocomputing}, 364:\penalty0 139--153, 2019{\natexlab{a}}.

\bibitem[George et~al.(2019{\natexlab{b}})George, Banerjee, and
  Suri]{george2019neuromorphic}
George, A.~M., Banerjee, D., and Suri, M.
\newblock Ieee 754 floating-point addition for neuromorphic architecture.
\newblock \emph{Neurocomputing}, 364:\penalty0 139--153, 2019{\natexlab{b}}.

\bibitem[Guo et~al.(2023)]{guo2023efficient}
Guo, W. et~al.
\newblock Efficient training of spiking neural networks with
  temporally-truncated backpropagation through time.
\newblock \emph{Frontiers in Neuroscience}, 17:\penalty0 1047008, 2023.

\bibitem[Kudithipudi et~al.(2025)]{kudithipudi2025neuromorphic}
Kudithipudi, D. et~al.
\newblock Neuromorphic computing at scale.
\newblock \emph{Nature}, 2025.

\bibitem[Kwak et~al.(2021)]{kwak2021precision}
Kwak, M. et~al.
\newblock Precision exploration of floating-point arithmetic for spiking neural
  networks.
\newblock In \emph{2021 IEEE International Conference on Artificial
  Intelligence Circuits and Systems (AICAS)}, pp.\  1--4. IEEE, 2021.

\bibitem[Li et~al.(2024)Li, Lei, and Yang]{li2024spikeformer}
Li, Y., Lei, Y., and Yang, X.
\newblock Spikeformer: Training high-performance spiking neural network with
  transformer.
\newblock \emph{Neurocomputing}, 580:\penalty0 127499, 2024.

\bibitem[Liu et~al.(2022)]{liu2022enhancing}
Liu, F. et~al.
\newblock Enhancing spiking neural networks with hybrid top-down attention
  mechanism.
\newblock \emph{Scientific Reports}, 12\penalty0 (1):\penalty0 15250, 2022.

\bibitem[Lu \& Xu(2022)Lu and Xu]{lu2022linear}
Lu, S. and Xu, F.
\newblock Linear leaky-integrate-and-fire neuron model based spiking neural
  networks and its mapping relationship to deep neural networks.
\newblock \emph{Frontiers in neuroscience}, 16:\penalty0 857513, 2022.

\bibitem[Malcolm et~al.(2023)]{malcolm2023comprehensive}
Malcolm, K. et~al.
\newblock A comprehensive review of spiking neural networks.
\newblock \emph{arXiv preprint arXiv:2303.10780}, 2023.

\bibitem[Marasco et~al.(2023)]{marasco2023adaptive}
Marasco, A. et~al.
\newblock An adaptive generalized leaky integrate-and-fire model for
  hippocampal ca1 pyramidal neurons and interneurons.
\newblock \emph{Cognitive Neurodynamics}, 17\penalty0 (5):\penalty0 1261--1288,
  2023.

\bibitem[Meng et~al.(2023)]{meng2023towards}
Meng, Q. et~al.
\newblock Towards memory-and time-efficient backpropagation for training
  spiking neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  6166--6176, 2023.

\bibitem[Mikaitis(2020)]{mikaitis2020arithmetic}
Mikaitis, M.
\newblock \emph{Arithmetic accelerators for a digital neuromorphic processor}.
\newblock PhD thesis, University of Manchester, 2020.

\bibitem[Neftci et~al.(2019)Neftci, Mostafa, and Zenke]{neftci2019surrogate}
Neftci, E.~O., Mostafa, H., and Zenke, F.
\newblock Surrogate gradient learning in spiking neural networks: Bringing the
  power of gradient-based optimization to spiking neural networks.
\newblock \emph{IEEE Signal Processing Magazine}, 36\penalty0 (6):\penalty0
  51--63, 2019.

\bibitem[Papamarkou et~al.(2024)]{papamarkou2024position}
Papamarkou, T. et~al.
\newblock Position: Topological deep learning is the new frontier for
  relational learning.
\newblock \emph{arXiv preprint arXiv:2402.08871}, 2024.

\bibitem[Rathi \& Roy(2023)Rathi and Roy]{rathi2023exploring}
Rathi, N. and Roy, K.
\newblock Exploring neuromorphic computing based on spiking neural networks:
  Algorithms to hardware.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (12):\penalty0 1--49, 2023.

\bibitem[Schuman et~al.(2017)Schuman, Potok, Patton, Birdwell, Dean, Rose, and
  Plank]{schuman2017survey}
Schuman, C.~D., Potok, T.~E., Patton, R.~M., Birdwell, J.~D., Dean, M.~E.,
  Rose, G.~S., and Plank, J.~S.
\newblock A survey of neuromorphic computing and neural networks in hardware.
\newblock \emph{arXiv preprint arXiv:1705.06963}, 2017.

\bibitem[Shao et~al.(2020)Shao, Hu, Wang, Xue, and Raj]{shao2020normalization}
Shao, J., Hu, K., Wang, C., Xue, X., and Raj, B.
\newblock Is normalization indispensable for training deep neural network?
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  13434--13444, 2020.

\bibitem[Shrestha \& Orchard(2022)Shrestha and Orchard]{shrestha2022survey}
Shrestha, A. and Orchard, G.
\newblock A survey on neuromorphic computing: Models and hardware.
\newblock \emph{IEEE Circuits and Systems Magazine}, 22\penalty0 (2):\penalty0
  6--35, 2022.

\bibitem[Suresh et~al.(2024)]{suresh2024characterizing}
Suresh, S. et~al.
\newblock On characterizing the evolution of embedding space of neural networks
  using algebraic topology.
\newblock \emph{Pattern Recognition Letters}, 180:\penalty0 107--114, 2024.

\bibitem[Teeter et~al.(2018)Teeter, Iyer, Menon, Gouwens, Feng, Berg, Szafer,
  Cain, Zeng, Hawrylycz, et~al.]{teeter2018generalized}
Teeter, C., Iyer, R., Menon, V., Gouwens, N., Feng, D., Berg, J., Szafer, A.,
  Cain, N., Zeng, H., Hawrylycz, M., et~al.
\newblock Generalized leaky integrate-and-fire models classify multiple neuron
  types.
\newblock \emph{Nature communications}, 9\penalty0 (1):\penalty0 709, 2018.

\bibitem[Wei et~al.(2025)Wei, Yang, Han, and Zhang]{wei2025physics}
Wei, Q., Yang, Q., Han, L., and Zhang, T.
\newblock Physics-informed spiking neural networks for continuous-time dynamic
  systems.
\newblock \emph{Neurocomputing}, 2025.

\bibitem[Wurm et~al.(2023)]{wurm2023arithmetic}
Wurm, A. et~al.
\newblock Arithmetic primitives for efficient neuromorphic computing.
\newblock In \emph{2023 IEEE International Conference on Rebooting Computing
  (ICRC)}, pp.\  1--8. IEEE, 2023.

\bibitem[Xu et~al.(2024)]{xu2024organic}
Xu, Z. et~al.
\newblock Organic frameworks memristor: Recent advances and perspectives for
  neuromorphic computing.
\newblock \emph{Advanced Materials}, 2024.

\bibitem[Yamazaki et~al.(2022)Yamazaki, Vo-Ho, Bulsara, and
  Le]{yamazaki2022spiking}
Yamazaki, K., Vo-Ho, V.-K., Bulsara, D., and Le, N.
\newblock Spiking neural networks and their applications: A review.
\newblock \emph{Brain Sciences}, 12\penalty0 (7):\penalty0 863, 2022.

\bibitem[Yao et~al.(2023)]{yao2022attention}
Yao, M. et~al.
\newblock Attention spiking neural networks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 45\penalty0 (8):\penalty0 9393--9410, 2023.

\bibitem[Yao et~al.(2024)]{yao2024spike}
Yao, M. et~al.
\newblock Spike-driven transformer v2: Meta spiking neural network architecture
  inspiring the design of next-generation neuromorphic chips.
\newblock \emph{arXiv preprint arXiv:2404.03663}, 2024.

\bibitem[Yao et~al.(2022)]{yao2022glif}
Yao, X. et~al.
\newblock Glif: A unified gated leaky integrate-and-fire neuron for spiking
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  32160--32171, 2022.

\bibitem[Yin et~al.(2019)Yin, Lyu, Zhang, Osher, Qi, and
  Xin]{yin2019understanding}
Yin, P., Lyu, J., Zhang, S., Osher, S., Qi, Y., and Xin, J.
\newblock Understanding straight-through estimator in training activation
  quantized neural nets.
\newblock \emph{arXiv preprint arXiv:1903.05662}, 2019.

\bibitem[Zenke \& Vogels(2021)Zenke and Vogels]{zenke2021remarkable}
Zenke, F. and Vogels, T.~P.
\newblock The remarkable robustness of surrogate gradient learning for
  instilling complex function in spiking neural networks.
\newblock \emph{Neural Computation}, 33\penalty0 (4):\penalty0 899--925, 2021.

\bibitem[Zhang et~al.(2021)]{zhang2021bifurcation}
Zhang, S. et~al.
\newblock Bifurcation spiking neural network.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (184):\penalty0 1--50, 2021.

\end{thebibliography}
