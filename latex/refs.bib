% MofNeuroSim References

%==============================================================================
% Neuromorphic Computing Surveys
%==============================================================================

@article{schuman2017survey,
  title={A survey of neuromorphic computing and neural networks in hardware},
  author={Schuman, Catherine D and Potok, Thomas E and Patton, Robert M and Birdwell, J Douglas and Dean, Mark E and Rose, Garrett S and Plank, James S},
  journal={arXiv preprint arXiv:1705.06963},
  year={2017}
}

@article{shrestha2022survey,
  title={A survey on neuromorphic computing: Models and hardware},
  author={Shrestha, Amar and Orchard, Garrick},
  journal={IEEE Circuits and Systems Magazine},
  volume={22},
  number={2},
  pages={6--35},
  year={2022},
  publisher={IEEE}
}

@article{kudithipudi2025neuromorphic,
  title={Neuromorphic computing at scale},
  author={Kudithipudi, Dhireesha and others},
  journal={Nature},
  year={2025},
  publisher={Nature Publishing Group}
}

%==============================================================================
% SNN Reviews
%==============================================================================

@article{yamazaki2022spiking,
  title={Spiking neural networks and their applications: A review},
  author={Yamazaki, Kashu and Vo-Ho, Viet-Khoa and Bulsara, Darshan and Le, Ngan},
  journal={Brain Sciences},
  volume={12},
  number={7},
  pages={863},
  year={2022},
  publisher={MDPI}
}

@article{rathi2023exploring,
  title={Exploring neuromorphic computing based on spiking neural networks: Algorithms to hardware},
  author={Rathi, Nitin and Roy, Kaushik},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--49},
  year={2023},
  publisher={ACM}
}

@article{malcolm2023comprehensive,
  title={A comprehensive review of spiking neural networks},
  author={Malcolm, Kaleb and others},
  journal={arXiv preprint arXiv:2303.10780},
  year={2023}
}

%==============================================================================
% Floating-Point in Neuromorphic Systems
%==============================================================================

@article{george2019ieee,
  title={IEEE 754 floating-point addition for neuromorphic architecture},
  author={George, Anup Mathew and Banerjee, Debayan and Suri, Manan},
  journal={Neurocomputing},
  volume={364},
  pages={139--153},
  year={2019},
  publisher={Elsevier}
}

@article{george2019neuromorphic,
  title={IEEE 754 floating-point addition for neuromorphic architecture},
  author={George, Anup Mathew and Banerjee, Debayan and Suri, Manan},
  journal={Neurocomputing},
  volume={364},
  pages={139--153},
  year={2019},
  publisher={Elsevier}
}

@article{dubey2020floating,
  title={Floating-point multiplication using neuromorphic computing},
  author={Dubey, Kshitij and Raj, Bharat and others},
  journal={arXiv preprint arXiv:2008.13245},
  year={2020}
}

@inproceedings{wurm2023arithmetic,
  title={Arithmetic primitives for efficient neuromorphic computing},
  author={Wurm, Alexander and others},
  booktitle={2023 IEEE International Conference on Rebooting Computing (ICRC)},
  pages={1--8},
  year={2023},
  organization={IEEE}
}

@phdthesis{mikaitis2020arithmetic,
  title={Arithmetic accelerators for a digital neuromorphic processor},
  author={Mikaitis, Mantas},
  year={2020},
  school={University of Manchester}
}

@inproceedings{kwak2021precision,
  title={Precision exploration of floating-point arithmetic for spiking neural networks},
  author={Kwak, Minhyeok and others},
  booktitle={2021 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)},
  pages={1--4},
  year={2021},
  organization={IEEE}
}

%==============================================================================
% GLIF and LIF Models
%==============================================================================

@article{teeter2018generalized,
  title={Generalized leaky integrate-and-fire models classify multiple neuron types},
  author={Teeter, Corinne and Iyer, Ramakrishnan and Menon, Vilas and Gouwens, Nathan and Feng, David and Berg, Jim and Szafer, Aaron and Cain, Nicholas and Zeng, Hongkui and Hawrylycz, Michael and others},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={709},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{yao2022glif,
  title={GLIF: A unified gated leaky integrate-and-fire neuron for spiking neural networks},
  author={Yao, Xingting and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32160--32171},
  year={2022}
}

@article{marasco2023adaptive,
  title={An adaptive generalized leaky integrate-and-fire model for hippocampal CA1 pyramidal neurons and interneurons},
  author={Marasco, Addolorata and others},
  journal={Cognitive Neurodynamics},
  volume={17},
  number={5},
  pages={1261--1288},
  year={2023},
  publisher={Springer}
}

@article{lu2022linear,
  title={Linear leaky-integrate-and-fire neuron model based spiking neural networks and its mapping relationship to deep neural networks},
  author={Lu, Sijia and Xu, Feng},
  journal={Frontiers in neuroscience},
  volume={16},
  pages={857513},
  year={2022},
  publisher={Frontiers}
}

%==============================================================================
% Dynamical Systems and Topology
%==============================================================================

@article{zhang2021bifurcation,
  title={Bifurcation spiking neural network},
  author={Zhang, Shaoqing and others},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={184},
  pages={1--50},
  year={2021}
}

@article{wei2025physics,
  title={Physics-informed spiking neural networks for continuous-time dynamic systems},
  author={Wei, Qian and Yang, Qiang and Han, Liang and Zhang, Tao},
  journal={Neurocomputing},
  year={2025},
  publisher={Elsevier}
}

@article{papamarkou2024position,
  title={Position: Topological deep learning is the new frontier for relational learning},
  author={Papamarkou, Theodore and others},
  journal={arXiv preprint arXiv:2402.08871},
  year={2024}
}

@article{suresh2024characterizing,
  title={On characterizing the evolution of embedding space of neural networks using algebraic topology},
  author={Suresh, Suchismit and others},
  journal={Pattern Recognition Letters},
  volume={180},
  pages={107--114},
  year={2024},
  publisher={Elsevier}
}

%==============================================================================
% SNN Training Methods
%==============================================================================

@article{neftci2019surrogate,
  title={Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks},
  author={Neftci, Emre O and Mostafa, Hesham and Zenke, Friedemann},
  journal={IEEE Signal Processing Magazine},
  volume={36},
  number={6},
  pages={51--63},
  year={2019},
  publisher={IEEE}
}

@article{zenke2021remarkable,
  title={The remarkable robustness of surrogate gradient learning for instilling complex function in spiking neural networks},
  author={Zenke, Friedemann and Vogels, Tim P},
  journal={Neural Computation},
  volume={33},
  number={4},
  pages={899--925},
  year={2021},
  publisher={MIT Press}
}

@article{yin2019understanding,
  title={Understanding straight-through estimator in training activation quantized neural nets},
  author={Yin, Penghang and Lyu, Jiancheng and Zhang, Shuai and Osher, Stanley and Qi, Yingyong and Xin, Jack},
  journal={arXiv preprint arXiv:1903.05662},
  year={2019}
}

@article{chen2024memristive,
  title={Memristive leaky integrate-and-fire neuron and learnable straight-through estimator in spiking neural networks},
  author={Chen, Tao and She, Chengdong and Wang, Lidan and Duan, Shukai},
  journal={Cognitive Neurodynamics},
  year={2024},
  publisher={Springer}
}

@article{guo2023efficient,
  title={Efficient training of spiking neural networks with temporally-truncated backpropagation through time},
  author={Guo, Wenshuo and others},
  journal={Frontiers in Neuroscience},
  volume={17},
  pages={1047008},
  year={2023},
  publisher={Frontiers}
}

@inproceedings{meng2023towards,
  title={Towards memory-and time-efficient backpropagation for training spiking neural networks},
  author={Meng, Qingyan and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6166--6176},
  year={2023}
}

%==============================================================================
% Attention and Transformers
%==============================================================================

@article{yao2022attention,
  title={Attention spiking neural networks},
  author={Yao, Man and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={8},
  pages={9393--9410},
  year={2023},
  publisher={IEEE}
}

@article{liu2022enhancing,
  title={Enhancing spiking neural networks with hybrid top-down attention mechanism},
  author={Liu, Fangxin and others},
  journal={Scientific Reports},
  volume={12},
  number={1},
  pages={15250},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{yao2024spike,
  title={Spike-driven transformer v2: Meta spiking neural network architecture inspiring the design of next-generation neuromorphic chips},
  author={Yao, Man and others},
  journal={arXiv preprint arXiv:2404.03663},
  year={2024}
}

@article{li2024spikeformer,
  title={Spikeformer: Training high-performance spiking neural network with transformer},
  author={Li, Yudong and Lei, Yunlin and Yang, Xu},
  journal={Neurocomputing},
  volume={580},
  pages={127499},
  year={2024},
  publisher={Elsevier}
}

%==============================================================================
% Normalization
%==============================================================================

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{shao2020normalization,
  title={Is normalization indispensable for training deep neural network?},
  author={Shao, Jie and Hu, Kai and Wang, Changhu and Xue, Xiangyang and Raj, Bhiksha},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13434--13444},
  year={2020}
}

%==============================================================================
% Neuromorphic Hardware
%==============================================================================

@article{davies2021advancing,
  title={Advancing neuromorphic computing with loihi: A survey of results and outlook},
  author={Davies, Mike and others},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={911--934},
  year={2021},
  publisher={IEEE}
}

%==============================================================================
% Encoding Methods
%==============================================================================

@article{date2023encoding,
  title={Encoding integers and rationals on neuromorphic computers using virtual neuron},
  author={Date, Prasanna and others},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={8987},
  year={2023},
  publisher={Nature Publishing Group}
}

%==============================================================================
% MOF Materials
%==============================================================================

@article{xu2024organic,
  title={Organic Frameworks Memristor: Recent advances and perspectives for neuromorphic computing},
  author={Xu, Zhonghui and others},
  journal={Advanced Materials},
  year={2024},
  publisher={Wiley}
}

@article{bachinin2024mof,
  title={Metal-organic framework single crystal for in-memory neuromorphic computing},
  author={Bachinin, Sergey V and others},
  journal={Nature Communications},
  year={2024},
  publisher={Nature Publishing Group}
}
