\section{Conclusion}
\label{sec:conclusion}
We presented NEXUS, a framework that achieves \textbf{bit-exact} ANN-to-SNN equivalence by constructing IEEE-754 compliant floating-point arithmetic entirely from IF neuron logic gates. Through spatial bit encoding (zero encoding error), hierarchical neuromorphic gate circuits (exact arithmetic from Level~1 logic gates to Level~4 nonlinear functions), and surrogate-free STE training (exact identity mapping), NEXUS produces \textbf{identical} outputs to standard ANNs. Experiments on models up to LLaMA-2 70B confirm identical task accuracy with mean ULP error of only 6.19, while achieving 27--168,000$\times$ energy reduction on neuromorphic hardware. NEXUS demonstrates that bit-exact SNN computation, energy efficiency, and robustness under physical hardware non-idealities are not mutually exclusive---spatial bit encoding's single-timestep design renders the framework inherently immune to membrane leakage (100\% accuracy at $\beta = 0.1$) while tolerating realistic synaptic noise levels ($\sigma \leq 0.2$ with ${>}98\%$ gate accuracy).