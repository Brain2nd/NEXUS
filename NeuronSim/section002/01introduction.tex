\section{Introduction}
\label{sec:introduction}

% With the continuous expansion of parameter scales in large-scale multimodal ANNs such as language and vision models, the demand for computational resources and energy consumption is also increasing exponentially~\citep{tay2022efficient}. Although the transformer has become the standard, it still suffers from issues during the inference stage, such as redundant activations, low sparsity, high bandwidth usage, and high energy consumption~\citep{tay2022efficient}. These challenges have prompted researchers to reexamine more biologically inspired, sparsity-friendly neural network architectures~\citep{roy2019towards}. 


% Spiking Neural Networks (SNNs), known as the third generation of neural networks (Maass, 1996), are promising due to their low computational cost and biological plausibility. Inspired by biological neurons, SNNs utilize binary spikes for communication between neurons instead of numerical values. Compared to traditional Artificial Neural Networks (ANNs), SNNs achieve higher energy efficiency due to the parsity of event-driven signaling, particularly on neuromorphic hardware (Davies et al., 2018; DeBole et al., 2019; Peiet al., 2019).
% Among them, 
% Spiking Neural Networks (SNNs), with their event-driven, asynchronous computation and naturally sparse mechanisms, are considered an important direction for building highly energy-efficient neural network systems~\citep{roy2019towards}.
% Especially on neuromorphic chips such as Loihi, SNNs demonstrate better energy efficiency and lower latency compared to traditional ANNs~\citep{davies2018loihi,davies2021loihi}. 
% However, limited by their information expression capability, such as the complexity of information encoding and the difficulty of training, SNNs are still mainly applied to light image recognition or signal detection and are difficult to handle large-scale sequence modeling or natural language processing~\citep{roy2019towards,zheng2021going}.
% In recent years, exploratory works such as Spikeformer~\citep{} and SpikeGPT~\citep{} have attempted to scale SNNs to larger modeling scenarios and shown initial potential~\citep{zhou2023spikformer,zhu2023spikegpt,yao2023sdt}. 

% SNN 扩展到大模型有什么问题？
% tzz第一段背景：脉冲神经网络（Spiking Neural Networks, SNNs）作为第三代受生物神经系统启发的神经网络，因其事件驱动和高度稀疏的计算特性而受到广泛关注。与传统人工神经网络（Artificial Neural Networks, ANNs）依赖连续激活值进行全局同步计算不同，SNNs 通过离散脉冲在时间维度上传递信息，仅在事件发生时触发计算，从而在能效面展现出显著优势。尤其在类脑芯片等新型硬件上，SNNs 能够充分发挥稀疏与异步特性，为构建低功耗、可扩展的大规模智能系统提供了潜在路径。
% Spiking Neural Networks (SNNs), known as the third generation of neural networks inspired by biological neural systems~\citep{maass1997networks}, have attracted increasing attention due to their event-driven and highly sparse computational characteristics. Unlike traditional Artificial Neural Networks (ANNs), which rely on continuous activation values for synchronous and global computation~\citep{lecun2015deep}, SNNs transmit information through discrete spikes along the temporal dimension and trigger computation only when events occur. This unique mechanism leads to remarkable advantages in energy efficiency~\citep{roy2019towards}. Especially on neuromorphic chips such as Loihi, SNNs can fully exploit sparsity and asynchrony, demonstrating superior energy efficiency and lower latency compared to ANNs~\citep{davies2018loihi,davies2021loihi}, and providing a promising path toward low-power and scalable large-scale intelligent systems.

% 脉冲神经网络（Spiking Neural Networks, SNNs）因其事件驱动的稀疏性而被认为在能效与大规模智能系统中具有潜力~\citep{maass1997networks,roy2019towards,davies2018loihi,davies2021loihi}。
% SNNs在大规模建模场景中的核心问题是脉冲编码、非线性计算误差累积与梯度近似导致信息表达效率低、训练收敛慢且不稳定。LASER的核心思想是跳出现有对连续值近似的编码思维，提出一种全新的脉冲表达语言，实现离散脉冲序列和连续性的对应双向映射；对于非线性计算，引入可学习的结构化脉冲编码器，把非线性带来的近似误差作为可校准的残差吸纳到编码空间并重编码，其余网络保持线性算子与拓扑不变，实现高保真且结构一致的前向传播；在高保真且结构一致的前向传播基础上，我们可以使用STE使网络可训练而不需使用梯度近似。此时STE不再是“启发式替代项”，而是前向一致的等价映射，所以误差极小、收敛稳定。

% Spiking Neural Networks (SNNs) are promising for energy-efficient large-scale intelligence due to event-driven sparsity~\citep{maass1997networks,roy2019towards,davies2018loihi,davies2021loihi}. 

% 一句话解释一下现有SNN架构是怎么玩的
% Their main challenges lie in spike encoding, nonlinear error accumulation, and gradient approximation, 
% causing low representational efficiency and unstable training. LASER addresses these by introducing a spike expression language with exact bidirectional mapping between spikes and continuous values, and a learnable structured spike encoder that localizes nonlinear errors while keeping the rest of the network linear and topologically intact. With such high-fidelity forward consistency, the Straight-Through Estimator (STE) serves as an equivalent mapping rather than a heuristic, yielding minimal error and stable convergence.
% Spiking Neural Networks (SNNs) are considered promising for energy efficiency and large-scale intelligent systems due to their event-driven sparsity~\citep{maass1997networks,roy2019towards,davies2018loihi,davies2021loihi}.  
% In large-scale modeling, the core challenges of SNNs lie in spike encoding, error accumulation in nonlinear computation, and gradient approximation, which lead to low information representation efficiency, slow convergence, and unstable training.  
% The key idea of LASER is to break away from the conventional paradigm of approximating continuous values, and instead propose a new spike expression language that establishes a bidirectional mapping between discrete spike sequences and continuous values.  
% For nonlinear computation, a learnable structured spike encoder is introduced to absorb approximation errors from nonlinearities as calibratable residuals into the encoding space and re-encode them, while keeping the rest of the network linear and topologically unchanged, thereby achieving high-fidelity and structurally consistent forward propagation.  
% Based on this, the Straight-Through Estimator (STE) can be applied to make the network trainable without gradient approximation. In this setting, STE is no longer a “heuristic substitute” but a forward-consistent equivalent mapping, leading to minimal error and stable convergence.
% 上述问题并非只在某一个环节产生，而是会在编码、非线性和训练的每个环节都额外引入误差，并且这些误差会逐层累积。具体来说：(i) 线性编码方面，离散值近似连续纸本身本身就是近似过程，需要长时间积累才能逼近数值，因此在一开始就产生基础误差，并随网络传播扩散累积；(ii) 非线性处理方面，ReLU、Softmax等算子的脉冲实现缺乏高精度的近似方式，每一次非线性运算都会引入新的累积误差；(iii) 训练方面，离散脉冲的不可导性迫使现有方法依赖 surrogate gradient 或软替代，这些近似本身也会带来新的在网络中扩散误差。
Spiking Neural Networks (SNNs) are regarded as the third generation of neural network models due to their unique neural dynamics and high biological plausibility, making them competitive candidates to Artificial Neural Networks (ANNs)~\citep{maass1997networks,roy2019towards,davies2018loihi,davies2021loihi}. However, existing SNN architectures face issues of low representational efficiency and unstable training. This mainly arises from the inaccurate approximation of continuous activations by discrete spikes~\citep{auge2021encoding,guo2021coding}. Specifically, during the encoding stage of converting ANNs into SNN spike representations, the use of discrete spikes to represent continuous values inherently introduces errors. To reduce this error, longer timesteps are required, leading to higher computational latency. As shown in Table~\ref{tab:bse_fidelity}, for the classic Time-To-First-Spike encoding~\citep{bonilla2022ttfs,stanojevic2024ttfs}, when the timestep is 1024, the error is reduced by 14 orders of magnitude compared with 16, but the latency increases by 64 times. More critically, nonlinear computations lack a unified spike representation. Some works~\citep{zhou2023spikformer,yao2023sdt,spikedattention2024,wang2023stsa} propose replacing nonlinear operations with ``spike-friendly nonlinear structures'' to implement spike representations of these operations. However, this alters the network structure, introducing additional training alignment costs. Furthermore, to enable training of networks based on spike representations, support for backpropagation is required. The non-differentiability of spikes forces the use of surrogate gradients or soft substitutes~\citep{neftci2019surrogate,wu2018stbp,shrestha2018slayer,deng2022tet,zheng2021going}. Such approximation methods introduce extra errors, making training unstable.

% This issue arises in three key aspects and the resulting errors accumulate layer by layer~\citep{rueckauer2017conversion,neftci2019surrogate}: 


% mapping continuous values to discrete spikes is itself approximate and requires long timesteps to converge, thus introducing base errors from the start that accumulate during propagation (Table~\ref{tab:bse_fidelity})~\citep{rueckauer2017conversion,auge2021encoding,guo2021coding}; (ii) in nonlinear processing, spiking implementations of operators such as ReLU and Softmax lack precise approximations, so every nonlinear computation introduces new errors that further accumulate~\citep{rueckauer2017conversion,you2024spikeziptf,yao2023sdt,spikedattention2024}; (iii) in training, the non-differentiability of spikes forces the use of surrogate gradients or soft substitutes, which themselves introduce additional errors that diffuse through the network~\citep{neftci2019surrogate,wu2018stbp,shrestha2018slayer,deng2022tet,zheng2021going}.


% The fundamental problem of existing methods lies in the inaccurate approximation of continuous activations by discrete spikes~\citep{auge2021encoding,guo2021coding}. Such approximation is not confined to a single stage, but occurs in encoding, nonlinear processing, and training, where each stage introduces additional errors. These errors then accumulate layer by layer, forming error accumulation and diffusion, i.e., each layer generates its own error during computation, and the errors progressively accumulate and diffuse as they propagate forward and backward through the network~\citep{rueckauer2017conversion,neftci2019surrogate}. Specifically: (i) in terms of linear encoding, the mapping from continuous activations to discrete spikes is inherently approximate, requiring long timesteps to approach the target values. Thus, base errors are introduced from the very beginning and then accumulate and diffuse as they propagate through the network (Table~\ref{tab:bse_fidelity})~\citep{rueckauer2017conversion,auge2021encoding,guo2021coding}; (ii) in terms of nonlinear processing, spiking implementations of operators such as ReLU and Softmax lack high-precision approximations, so each nonlinear operation introduces new errors that further accumulate and diffuse~\citep{rueckauer2017conversion,you2024spikeziptf,yao2023sdt,spikedattention2024}; (iii) in terms of training, the non-differentiability of discrete spikes forces existing methods to rely on surrogate gradients or soft substitutes, and these approximations themselves introduce new errors that accumulate and diffuse through the network~\citep{neftci2019surrogate,wu2018stbp,shrestha2018slayer,deng2022tet,zheng2021going}.


% The fundamental problem of existing methods lies in the inaccurate approximation of continuous activations by discrete spikes~\citep{auge2021encoding,guo2021coding}. Such approximation does not occur in a single stage only, but introduces additional errors in encoding, nonlinear processing, and training, and these errors accumulate and diffuse progressively with network depth~\citep{rueckauer2017conversion,neftci2019surrogate}, 

% % as illustrated in Figure~\ref{fig:error_diffusion_localization}.
% Specifically: (i) in terms of linear encoding, the approximation of discrete values to continuous activations is itself an approximation process, requiring long timesteps to approach numerical values, thereby introducing base errors from the very beginning and accumulating as they propagate through the network (see Table~\ref{tab:bse_fidelity})~\citep{rueckauer2017conversion,auge2021encoding,guo2021coding}; (ii) in terms of nonlinear processing, spiking implementations of operators such as ReLU and Softmax lack high-precision approximation methods, so each nonlinear operation introduces new cumulative errors~\citep{rueckauer2017conversion,you2024spikeziptf,yao2023sdt,spikedattention2024}; (iii) in terms of training, the non-differentiability of discrete spikes forces existing methods to rely on surrogate gradients or soft substitutes, which themselves introduce new errors that further diffuse through the network~\citep{neftci2019surrogate,wu2018stbp,shrestha2018slayer,deng2022tet,zheng2021going}.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.85\linewidth]{figures/error_diffusion.pdf}
%     \caption{Comparison of error accumulation–diffusion and error localization: (a) Conventional methods introduce errors at multiple stages, which progressively accumulate and diffuse with network depth and timesteps; (b) In LASER, linear computations remain lossless, and errors are strictly confined to a single nonlinear module without layer-wise accumulation or diffusion; (c) Bit Spike Encoding (BSE) maps an $N$-bit integer to its corresponding $N$-bit binary spike sequence to achieve exact losslessness, and for floating-point representations it also preserves the same information entropy and machine precision as the original values (see Appendix~\ref{app:bse_entropy_proof}).
% }
%     \label{fig:error_diffusion_localization}
% \end{figure}

\begin{table}[t]
\centering
\caption{Reconstruction fidelity (MSE; mean $\pm$ std over 10{,}000 random values) with explicit precision and latency cost (Time Steps). BSE attains $10^{11}$–$10^{18}$ lower error than rate coding and TTFS under the same step budgets, reaching near-machine precision at FP32. Notably, TTFS requires as many as 1024 time steps to match the fidelity that BSE already achieves within 16 steps, highlighting the substantially higher latency cost of TTFS.}
\label{tab:bse_fidelity}
\begin{tabular}{lccc}
\hline
\textbf{Encoding Scheme} & \textbf{Time Steps} & \textbf{MSE (mean $\pm$ std)} \\
\hline
Rate Coding     & 16   & $7.70\pm 0.02 \times 10^{4}$ \\
Rate Coding     & 32   & $4.46\pm 0.01 \times 10^{4}$ \\
TTFS Former     & 16   & $3.68\pm 0.02 \times 10^{5}$ \\
TTFS Former     & 32   & $6.03\pm 0.01 \times 10^{4}$ \\
TTFS Former     & 1024 & $1.03\pm 0.01 \times 10^{-9}$ \\
\textbf{BSE (Ours)}& 2    & $\mathbf{9.26\pm 0.04 \times 10^{4}}$ \\
\textbf{BSE (Ours)}& 4    & $\mathbf{3.70\pm 0.03 \times 10^{-4}}$ \\
\textbf{BSE (Ours)}& 8    & $\mathbf{1.28\pm 0.01 \times 10^{-6}}$ \\
\textbf{BSE (Ours)}& 16   & $\mathbf{1.03\pm 0.02 \times 10^{-9}}$ \\
\textbf{BSE (Ours)}& 32   & $\mathbf{1.33\pm 0.01 \times 10^{-14}}$ \\
\hline
\end{tabular}
\end{table}

% 在应对这些问题时，现有方法往往以效率换取准确率：(i) 在线性编码上依赖较长时间窗来提升近似精度，但这会显著增加延迟和能耗，使得推理效率下降；(ii) 在非线性处理中通过结构改写或脉冲友好替代来维持模型功能与表达力，例如激活函数都替换为ReLU，但需要额外的计算和训练对齐开销，本质上同样牺牲了效率；(iii) 在训练阶段，使用时间反向传播（BackPropagation Through Time, BPTT）等方法结合梯度替代，提升训练稳定性。但BPTT必须存储所有时间步的状态，使得延迟和存储开销随网络规模急剧增长~\citep{wu2018stbp,shrestha2018slayer,neftci2019surrogate,deng2022tet,zheng2021going}。

In addressing these issues, existing methods often trade efficiency for accuracy: (i) in linear encoding, they rely on long time windows to improve approximation precision, but this significantly increases latency and energy consumption, reducing inference efficiency~\citep{guo2021coding,bu2023ultralow,you2024spikeziptf}; (ii) in nonlinear processing, they adopt structural rewrites or spike-friendly substitutes (e.g., replacing activation functions with ReLU) to maintain model functionality and expressiveness, but these introduce extra computation and training alignment overhead, essentially sacrificing efficiency as well~\citep{zhou2023spikformer,zhou2024spikformerv2,yao2023sdt,spikedattention2024,tang2024sorbet,you2024spikeziptf}; In the training stage, methods such as BackPropagation Through Time (BPTT) combined with surrogate gradients improve training stability. However, BPTT must store the states of all timesteps, causing latency and memory overhead to grow rapidly with network scale~\citep{wu2018stbp,shrestha2018slayer,neftci2019surrogate,deng2022tet,zheng2021going}.


% 因此，我们提出 LASER，一个误差有界的脉冲表示框架。该框架不做全局近似（下个定义，和前面反复提的误差累积对应起来），先以一种低延迟的精确双向映射编码方式，保证了离散脉冲与ANN连续激活值的一致性，同时使用分段自适应策略拟合非线性曲线，同时保持输出均为脉冲编码格式，从而不做结构上的牺牲完成脉冲空间里的端到端推理，误差固定在单一的非线性位置，使前向路径保持与ANN的等价。正是由于我们的高保真前向传播，可以使用STE替代导数而不改变数值路径，做到收敛稳定，误差不随时间与深度扩散。
% In this paper, Instead of applying global approximations, we propose LASER, an error-bounded spiking representation framework, 且不损失效率. 主要贡献如下：
% \begin{itemize}

% 现在的：
    % \item 我们首次提出离散脉冲与连续值的精确对应编码方式 Bit Spike Encoding (BSE)。其核心思想是通过同位宽量化将浮点数转化为整数，保持与原始数值相同的信息熵和机器精度（证明见附录~\ref{app:bse_entropy_proof}），并利用固定的短时间窗，将整数数值映射为与其二进制表示一致的脉冲序列。
    % \item 在 BSE 编码的基础上我们提出自适应脉冲编解码器（ASNC），用于非线性计算，与现有方法需要进行大规模结构替换不同，ASNC 作为结构化逼近器无需改写网络结构，即可在脉冲域完成任意非线性的局部近似，保持整体结构路径一致，无需训练对齐。
    % \item 在高保真的前向传播基础上，我们引入恒等梯度代理（STE）替代导数，实现了稳定收敛的脉冲神经网络训练。由于我们的前向传播没有对网络进行结构上的改变，因此梯度路径与数值路径完全对齐，反向传播不会额外改变计算过程，从而保证了训练的收敛稳定。



    %     \item 我们首先提出无损确定性脉冲编码 Bit Spike Encoding，而不是在既有压缩方案上做降低损失。BSE 的核心思想是利用固定的短时间窗，将一个数值拆解为稀疏有序的脉冲序列，每个时间步对应一位二进制比特。这样 $N$ 个时间步可以完整表示 Int$N$ 的所有数值范围，同时通过同位宽量化在浮点数表示上保持与原始数值相同的信息熵和机器精度（证明见附录~\ref{app:bse_entropy_proof}）。在这种编码下，数值的编码和解码互为确定过程，所有线性计算都可以直接在脉冲域执行，得到的结果与在原始数值域中的计算完全一致，因此不会引入额外误差。与之形成对比的是，传统的编码方式依赖长时间窗来提高近似精度，本身存在不可逆近似损失，导致误差会随着深度累积扩散。BSE 通过确定性和可逆性消除了线性计算这一扩散路径，为后续将误差固定在非线性近似模块、实现稳定训练和大规模扩展奠定基础。
    % \item 我们首先提出无损且确定性的脉冲编码 Bit Spike Encoding (BSE)，而非仅在既有压缩方案上减少损失。其核心思想是通过同位宽量化将浮点数转化为整数，保持与原始数值相同的信息熵和机器精度（证明见附录~\ref{app:bse_entropy_proof}），并利用固定的短时间窗，将 Int$N$ 数值映射为与其二进制表示一致的 $N$ 个时间步脉冲序列。在这种编码下，编码与解码互为确定过程，所有线性计算可直接在脉冲域执行且结果与数值域完全一致，从而消除线性计算中的误差扩散路径，为后续将误差固定在非线性近似模块并实现稳定训练与大规模扩展奠定基础。
    % \item 在 BSE 编码的基础上我们提出自适应脉冲编解码器（ASNC），并保证输出继续符合 BSE 编码格式，实现高保真的前向传播。ASNC 的核心思想是在非线性环节进行分段拟合，将输入域自适应地划分为多个区间，并在每个区间内用紧凑的脉冲编解码器近似目标函数。每个区间的近似结果会被立即重新编码为 BSE 脉冲序列，因此中间张量始终保持 BSE 格式，误差被严格限制在 ASNC 单元内部，而不会扩散到网络的其他部分。与现有方法需要进行大规模结构替换不同，ASNC 作为结构化逼近器无需改写网络结构，即可在脉冲域完成任意非线性的局部近似，同时保持整体路径的数值一致性。基于此，线性部分由 BSE 保证无损，非线性部分由 ASNC 局部化近似并严格隔离误差，二者结合共同构成了 LASER 的高保真且结构一致的前向传播机制，为后续使用恒等梯度代理（STE）进行稳定训练奠定坚实基础。
    % \item 在 BSE 编码的基础上我们提出自适应脉冲编解码器（ASNC），在非线性环节进行分段拟合，将输入域自适应地划分为多个区间，并在每个区间内用紧凑的脉冲编解码器近似目标函数，且近似结果会被立即重新编码为 BSE 脉冲序列。与现有方法需要进行大规模结构替换不同，ASNC 作为结构化逼近器无需改写网络结构，即可在脉冲域完成任意非线性的局部近似，保持整体结构路径一致，无需训练对齐。基于此，线性部分由 BSE 保证无损，非线性部分由 ASNC 局部化近似并严格隔离误差，二者结合共同构成了高保真且结构一致的前向传播机制，为后续使用恒等梯度代理（STE）进行稳定训练奠定坚实基础。
    % \item 在高保真的前向传播基础上，我们引入直通估计（STE）替代导数，此时恒等梯度代理不再是经验性或启发式的近似，而是与前向数值路径严格一致的等价近似。由于梯度路径与数值路径完全对齐，反向传播不会额外改变计算过程，从而保证了训练的收敛稳定，误差也不会累积扩散。
    % \end{itemize}
In this paper, we propose LASER, a surrogate-free spike representation framework with structural approximation and error-localized training. 
% 而不是分别针对编码,非线性操作的脉冲表示或反向传播进行单点优化,我们将整个SNN的生命周期看作一个完整的整体,系统性的提出一套框架, 首先设计精准的编码机制, 该编码机制不仅支持线性xx无损xx, 并且也支持任意非线性操作的脉冲表示. 其次, 基于编码后脉冲表示的SNN, 可以进行xxxx反向传播.
The key idea of LASER is to break away from the conventional paradigm of approximating continuous values, and instead propose a new spike expression language that establishes a bidirectional mapping between discrete spike sequences and continuous values.  
% For nonlinear computation, a learnable structured spike encoder is introduced to absorb approximation errors from nonlinearities as calibratable residuals into the encoding space and re-encode them, while keeping the rest of the network linear and topologically unchanged, thereby achieving high-fidelity and structurally consistent forward propagation.  
% Based on this, the Straight-Through Estimator (STE) can be applied to make the network trainable without gradient approximation. In this setting, STE is no longer a “heuristic substitute” but a forward-consistent equivalent mapping, leading to minimal error and stable convergence.
% LASER的核心思想是跳出现有对连续值近似的编码思维，提出一种全新的脉冲表达语言，实现离散脉冲序列和连续性的对应双向映射；对于非线性计算，引入可学习的结构化脉冲编码器，把非线性带来的近似误差作为可校准的残差吸纳到编码空间并重编码，其余网络保持线性算子与拓扑不变，实现高保真且结构一致的前向传播；在高保真且结构一致的前向传播基础上，我们可以使用STE使网络可训练而不需使用梯度近似。此时STE不再是“启发式替代项”，而是前向一致的等价映射，所以误差极小、收敛稳定。
The main contributions are as follows:
% 这里不展开了，high level就行

\begin{itemize}
    \item We first propose Bit Spike Encoding (BSE), an exact correspondence scheme between discrete spikes and continuous values. Its core idea is to transform floating-point numbers into integers through same-bitwidth quantization, preserving the same information entropy and machine precision as the original values (proof in Appendix~\ref{app:bse_entropy_proof}), and then map each integer to a spike sequence consistent with its binary representation within a fixed short time window.
    \item Building on BSE, we introduce the Adaptive Spiking Neural Codec (ASNC) for nonlinear computations. Unlike existing methods that require large-scale structural replacements, ASNC functions as a structural approximator that locally approximates arbitrary nonlinear functions in the spike domain without modifying the network architecture, keeping the overall structural path consistent and avoiding the need for training alignment.
    \item Building on the high-fidelity forward propagation, we introduce the Straight-Through Estimator (STE) to replace derivatives. In this case, the identity gradient surrogate is no longer an empirical or heuristic approximation, but an equivalent approximation strictly consistent with the forward numerical path. Since the gradient path is fully aligned with the numerical path, backpropagation does not introduce additional modifications to the computation process, thereby ensuring stable convergence in training and preventing error accumulation and diffusion.

    % \item We first propose a lossless and deterministic spiking code, Bit Spike Encoding (BSE), rather than merely reducing the losses of existing compression-based schemes. The core idea of BSE is to use a fixed short time window to decompose a value into a sparse and ordered spike sequence, where each timestep corresponds to one binary bit. In this way, $N$ timesteps can fully represent the entire range of Int$N$, while through equal-bitwidth quantization BSE preserves the same information entropy and machine precision as the original floating-point value (see Appendix~\ref{app:bse_entropy_proof}). Under this coding, encoding and decoding are deterministic inverses, and all linear computations can be directly executed in the spiking domain with results exactly matching those in the numerical domain, thereby introducing no extra error. In contrast, traditional coding schemes such as rate coding and TTFS rely on long time windows to improve approximation precision, are inherently irreversible or lossy, and cause errors to accumulate and diffuse with depth~\citep{rueckauer2017conversion,auge2021encoding,guo2021coding}. BSE, by emphasizing determinism and reversibility, eliminates this diffusion path in linear computations, and thus lays the foundation for confining errors only to nonlinear approximation modules and enabling stable training at large scales.
    % \item We first propose a lossless and deterministic spiking code, Bit Spike Encoding (BSE), to  
    % 我们首次提出离散脉冲与连续值的精确对应编码方式，Bit Spike Encoding (BSE). 
    % rather than merely reducing the loss of existing compression schemes. 
    % The core idea is to quantize floating-point values into integers with equal bitwidth, thereby preserving the same information entropy and machine precision as the original values (see Appendix~\ref{app:bse_entropy_proof}). Using a fixed short time window, an Int$N$ value is mapped into an $N$-timestep spike sequence identical to its binary representation. Under this coding, encoding and decoding are deterministic inverses, and all linear computations can be executed directly in the spiking domain with results exactly matching those in the numerical domain. This removes error diffusion paths in linear operations and lays the foundation for confining errors to nonlinear approximation modules, enabling stable training and large-scale scalability.
    % \item Building upon BSE, we propose the Adaptive Spiking Neural Codec (ASNC), and ensure that its outputs remain in BSE format, thereby achieving high-fidelity forward propagation. The core idea of ASNC is to perform piecewise fitting at nonlinear operations: the input domain is adaptively partitioned into segments, and each segment is approximated with a compact spiking codec. The approximation result of each segment is immediately re-encoded into a BSE spike sequence, so that all intermediate tensors remain in BSE format and errors are strictly confined within the ASNC unit, without diffusing into other parts of the network. Unlike existing methods that require large-scale structural rewrites~\citep{zhou2023spikformer,zhou2024spikformerv2,yao2023sdt,spikedattention2024}, ASNC serves as a structural approximator that performs localized nonlinear approximation directly in the spiking domain while preserving overall numerical consistency. Based on this, the linear part is guaranteed to be lossless by BSE, and the nonlinear part is locally approximated and strictly error-isolated by ASNC. Together they form the high-fidelity and structurally consistent forward propagation mechanism of LASER, which establishes a solid foundation for stable training with the Straight-Through Estimator (STE).
    % \item Building on BSE, we propose the Adaptive Spiking Neural Codec (ASNC), which performs piecewise approximation at nonlinear modules. ASNC adaptively partitions the input domain into multiple intervals, and within each interval employs a compact spiking codec to approximate the target function. The approximated output is immediately re-encoded into a BSE spike sequence. Unlike existing approaches that require large-scale structural substitutions, ASNC acts as a structural approximator that enables local nonlinear approximation directly in the spiking domain without modifying the network architecture, thus preserving the overall structural path and eliminating the need for training alignment. Consequently, the linear part is guaranteed to be lossless by BSE, while the nonlinear part is locally approximated by ASNC with strictly isolated error. Together they form a high-fidelity and structurally consistent forward propagation mechanism, providing a solid foundation for stable training with straight-through estimators (STE).
    % \item On top of the high-fidelity forward propagation, we introduce STE as a surrogate for gradients, where the identity gradient proxy is no longer an empirical or heuristic approximation, but an equivalent approximation strictly consistent with the forward numerical path~\citep{wu2018stbp,shrestha2018slayer,neftci2019surrogate,deng2022tet,zheng2021going}. Since the gradient path is fully aligned with the numerical path, backpropagation does not alter the computation process, thereby ensuring convergence stability and preventing errors from accumulating and diffusing along timesteps or network depth.
\end{itemize}

% Instead of applying global approximations, the framework employs a low-latency, precise bidirectional mapping to ensure consistency between discrete spikes and continuous ANN activations, and adopts a piecewise adaptive strategy for nonlinear functions while keeping outputs in the spike domain. In this way, end-to-end inference in the spiking space is achieved without structural sacrifices, and approximation is strictly confined to a single nonlinear unit, preserving the equivalence of the forward path to the ANN. Owing to this high-fidelity forward propagation, STE can be applied as a surrogate derivative without altering the numerical path, ensuring stable convergence and preventing errors from diffusing across temporal and depth dimensions.

% 我们的实验系统性地验证了本方法的有效性。我们的编码方式在相同时间步长下，相比baseline误差低约 $10^{11}$ 到 $10^{18}$ 倍。消融实验中FFN 端到端偏差仅 $9.5 \times 10^{-7}$，验证了误差被限制在非线性计算单点。在大模型对比中，我们的方法在 LLaMA-2 7B 上困惑度升高为基线方法的0.05%，在70B参数量级端到端实验中，整体精度降幅小于 $2\%$，体现了结构一致性的可扩展优势。在神经形态硬件Loihi 2 上的能耗相比 GPU 下降了 $98\%$。
Our experiments systematically validate the effectiveness of our method. 
% Under the same number of time steps, our encoding yields errors about $10^{11}$ to $10^{18}$ times lower than the baseline. 
% The deviation of the FFN is only $9.5\times10^{-7}$, confirming that error is confined to nonlinear computation. 
As a result, in large-model comparisons, on LLaMA-2~7B the perplexity increase is $0.05\%$ of that of the baseline method; at the 70B scale in end-to-end experiments, the overall accuracy drop is below $2\%$.
% reflecting the scalability afforded by structural consistency. 
On neuromorphic hardware, energy consumption on Loihi~2 decreases $98\%$ comparing to that on the GPU.


% tzz第二段现有方法问题：然而，现有的 SNN 构建方法仍面临核心挑战。首先，常用的脉冲编码方式（如速率编码、时间编码）在近似连续数值时往往引入较大误差，并导致推理时需要较长的时间窗，从而限制了建模精度与延迟性能。其次，在激活函数、归一化等非线性运算的脉冲化过程中，误差不仅难以避免，还会沿着时间步与网络深度累积和扩散，虽然现有方法尝试使用脉冲友好的非线性组件进行替代并取得了初步进展（如使用relu替换silu），但这削弱了整体的结构一致性与可解释性。最后，依赖替代梯度（surrogate gradient）等“软替代”的训练策略，尽管缓解了不可微问题，但容易带来训练不稳定甚至收敛困难。这些因素共同导致当前 SNNs 难以在大规模任务中同时兼顾高精度与高能效，限制了其工程化部署。
% However, current approaches to constructing SNNs still face three main challenges. 
% 第一，ANN转换为SNN。经典的通过xxx近似的xxx方法，例如rate coding and temporal coding，不可避免的引入误差。为了缓解误差，通常xxx，但是xxx。对于非线性
% 第二、转换方法
% First, commonly used neural coding schemes such as rate coding~\citep{rueckauer2017conversion,auge2021encoding} and Time-To-First-Spike coding~\citep{bonilla2022ttfs,stanojevic2024ttfs} inevitably introduce significant errors when approximating continuous values, and often require long time windows during inference, thereby limiting both accuracy and latency performance~\citep{guo2021coding,bu2023ultralow}. Second, in the spiking implementation of nonlinear operations such as activation functions and normalization, errors are unavoidable and tend to accumulate and diffuse across timesteps and network depth. Although some studies attempt to substitute spike-friendly nonlinear components and achieve preliminary progress—for example, replacing SiLU with ReLU in spiking Transformers~\citep{zhou2023spikformer,zhou2024spikformerv2}—these substitutions undermine structural consistency and interpretability. Finally, training strategies that rely on surrogate gradients~\citep{neftci2019surrogate,wu2018stbp,shrestha2018slayer} provide a “soft substitute” to alleviate non-differentiability, but they often introduce unstable training dynamics or even convergence failure in large networks~\citep{zheng2021going,deng2022tet}. Together, these issues hinder SNNs from achieving both high accuracy and high energy efficiency at scale, thereby limiting their practical deployment~\citep{roy2019towards}.

% tzz3：近年来，大规模 SNN 模型在 Transformer 和 LLM 任务上的探索取得了一定进展，但普遍面临一个核心问题：无论采用何种策略，都会造成误差扩散和训练不稳定。例如，依赖较长的时间窗或较高的脉冲发放率~\citep{you2024spikeziptf}，虽然能提升近似精度，但会在时序维度引入更多累积误差；采用替代梯度~\citep{neftci2019surrogate,wu2018stbp}或近似可微的激活函数，则会让误差在反向传播中不断弥散，破坏训练稳定性；在非线性和注意力等关键结构中进行“脉冲友好”改写，如 Spikformer 与 Spikformer-V2 用自适应注意力替代标准注意力~\citep{zhou2023spikformer,zhou2024spikformerv2}，SpikeGPT 与 Spiking-LLM 借助软门控与近似脉冲单元进行自回归建模~\citep{zhu2023spikegpt,xing2025spikellm}，Spike-Driven Transformer 与基于脉冲的自注意力方法在 Q/K/V 路径和归一化环节做结构调整~\citep{yao2023sdt,spikedattention2024,wang2023stsa}，以及 SpikeZIP/SpikeZIP-TF 通过长时间窗与重参数化来对齐 ANN~\citep{you2024spikeziptf}，都在一定程度上缓解了不可微或结构适配问题，但最终仍未能避免误差沿时间与深度扩散，从而导致训练不稳。即便是尝试直接从零训练的 SpikeLM~\citep{xing2024spikelm}，也同样受到这一主线问题的制约。由此可见，在超大规模与严格时间步预算的条件下，现有方法难以同时保持结构一致性与高保真精度，工程化部署受限。因而亟需一条系统化路线，能够在最大程度保留信息表达力和非线性建模能力的同时，将误差局限在结构内可控的单点位置。
% tzz3:In recent years, initial explorations of large-scale SNN models have shown promising potential on Transformer and LLM tasks. However, these methods often rely on long time windows or high firing rates~\citep{you2024spikeziptf}, surrogate gradients~\citep{neftci2019surrogate,wu2018stbp}, approximately differentiable activations, and other “soft substitutes” to obtain trainability. Such approaches do not confine errors strictly to single modules (e.g., nonlinear units), and the resulting errors tend to diffuse along both temporal and depth dimensions, leading to unstable training and degraded accuracy at scale. Under strict timestep budgets and ultra-large model sizes, it remains difficult to simultaneously preserve strong structural consistency with the original ANN and maintain high-fidelity accuracy, which severely limits deployability. For example, Spikformer and Spikformer-V2 replace standard attention with spike-friendly self-attention variants~\citep{zhou2023spikformer,zhou2024spikformerv2}; SpikeGPT and Spiking-LLM introduce soft gating and approximately differentiable spiking units for autoregressive modeling~\citep{zhu2023spikegpt,xing2025spikellm}; Spike-Driven Transformer and spike-based self-attention approaches perform structural rewrites in Q/K/V paths and normalization~\citep{yao2023sdt,spikedattention2024,wang2023stsa}; SpikeZIP and SpikeZIP-TF achieve ANN-SNN alignment through long time windows, reparameterization, or quantization equivalence, but still require spike-friendly activations for stable conversion~\citep{you2024spikeziptf}; and SpikeLM explores direct training from scratch~\citep{xing2024spikelm}. These limitations highlight the need for a principled roadmap toward ideal large-scale SNNs, with the core objective of preserving information expressivity, nonlinear modeling capacity, and tunability, while localizing inevitable errors to predictable single points within the network structure.

% tzz4：这些方法共同的问题是存在误差随时间与深度扩散，导致模型规模增大时误差累计严重，精度丢失，不得不在时间延迟或网络结构上进行妥协。因此，我们提出 LASER，一个误差有界的脉冲表示框架。该框架不做全局近似，先以一种低延迟的精确双向映射编码方式，保证了离散脉冲与ANN连续激活值的一致性，同时使用分段自适应策略拟合非线性曲线，同时保持输出均为脉冲编码格式，从而不做结构上的牺牲完成脉冲空间里的端到端推理，误差固定在单一的非线性位置，使前向路径保持与ANN的等价。正是由于我们的高保真前向传播，可以使用STE替代导数而不改变数值路径，做到收敛稳定，误差不随时间与深度扩散。
% tzz4:The common limitation of existing approaches is that approximation errors diffuse over time and depth, leading to severe accumulation as model scale increases, accuracy degradation, and inevitable compromises in latency or network structure. To address this, we propose LASER, an error-bounded spiking representation framework. Instead of applying global approximations, the framework employs a low-latency, precise bidirectional mapping to ensure consistency between discrete spikes and continuous ANN activations, and adopts a piecewise adaptive strategy for nonlinear functions while keeping outputs in the spike domain. In this way, end-to-end inference in the spiking space is achieved without structural sacrifices, and approximation is strictly confined to a single nonlinear unit, preserving the equivalence of the forward path to the ANN. Owing to this high-fidelity forward propagation, STE can be applied as a surrogate derivative without altering the numerical path, ensuring stable convergence and preventing errors from diffusing across temporal and depth dimensions.

% tzz5:我们的实验系统性地验证了本方法能将近似误差固定在唯一可控的非线性模块，并保持前向与反向路径的结构一致性。基于此，网络训练更稳定、可扩展，并具备显著的能效优势。首先，在编码层面，BSE 在相同步长下的误差比 rate coding 和 TTFS 低约 $10^{11}$ 到 $10^{18}$ 倍，证明了其作为高保真双向映射的有效性。其次，在系统层面的消融实验中，误差被严格局限在 ASNC 单点，FFN 端到端偏差仅 $9.5\times10^{-7}$，验证了误差的局部化机制。其后，在端到端层面，我们在 MMLU、HellaSwag、ARC、TruthfulQA 等基准上保持不少于 $98\%$ 的 ANN 性能，其中 LLaMA-2 70B 的 MMLU 仅下降 $1.2\%$，整体精度降幅小于 $2\%$。进一步，在大模型对比中，我们的方法在 LLaMA-2 7B 上困惑度仅比 ANN 高 $0.46$，为基线方法的0.05%，体现了结构一致性的可扩展优势。最后，在硬件层面，Loihi 2 上的能耗约为 GPU 的 $2\%$。这些结果共同表明，LASER 在保证结构一致性的同时，实现了误差局部化与高效可扩展性。
% tzz5:Our experiments systematically validate that our method can confine approximation error to a single controllable nonlinear module and maintain structural consistency of the forward and backward paths. On this basis, network training is more stable, more scalable, and exhibits significant energy-efficiency advantages. First, at the encoding level, under the same time-step budget, BSE yields errors about $10^{11}$ to $10^{18}$ times lower than rate coding and TTFS, demonstrating its effectiveness as a high-fidelity bidirectional mapping. Second, in system-level ablations, error is strictly confined to the ASNC unit, and the end-to-end deviation of the FFN is only $9.5\times10^{-7}$, verifying the error-localization mechanism. Next, at the end-to-end level, across MMLU, HellaSwag, ARC, and TruthfulQA, we retain no less than $98\%$ of the ANN performance, with LLaMA-2 70B dropping by only $1.2\%$ on MMLU and overall accuracy degradation under $2\%$. Furthermore, in large-model comparisons on LLaMA-2 7B, our perplexity is higher than the ANN baseline by only $0.46$, which is $0.05\%$ of the baseline method, reflecting the scalability afforded by structural consistency. Finally, at the hardware level, the energy consumption on Loihi~2 is about $2\%$ of that on the GPU. These results collectively show that LASER achieves error localization and efficient scalability while preserving structural consistency.

% tzz6:我们的实验系统性地验证了本方法能将近似误差固定在唯一可控的非线性模块，并保持前向与反向路径的结构一致性。基于此，网络训练更稳定、可扩展，并具备显著的能效优势。我们的编码方式在相同时间步长下，相比baseline误差低约 $10^{11}$ 到 $10^{18}$ 倍。消融实验中FFN 端到端偏差仅 $9.5 \times 10^{-7}$，验证了误差被限制在非线性计算单点。在大模型对比中，我们的方法在 LLaMA-2 7B 上困惑度升高为基线方法的0.05%，在70B参数量级端到端实验中，整体精度降幅小于 $2\%$，体现了结构一致性的可扩展优势。在神经形态硬件Loihi 2 上的能耗约为 GPU 的 $2\%$。
% tzz6:Our experiments systematically validate that our method can confine approximation error to a single controllable nonlinear module and maintain structural consistency between the forward and backward paths. On this basis, network training is more stable, more scalable, and exhibits significant energy-efficiency advantages. Under the same number of time steps, our encoding yields errors about $10^{11}$ to $10^{18}$ times lower than the baseline. In ablation studies, the end-to-end deviation of the FFN is only $9.5 \times 10^{-7}$, confirming that error is restricted to a single nonlinear computation unit. In large-model comparisons, on LLaMA-2~7B the perplexity increase is $0.05\%$ of the baseline method; at the 70B scale, the overall end-to-end accuracy drop is below $2\%$, reflecting the scalability afforded by structural consistency. On neuromorphic hardware, energy consumption on Loihi~2 is about $2\%$ of that on the GPU.

% tzz7:我们的实验系统性地验证了本方法的有效性。我们的编码方式在相同时间步长下，相比baseline误差低约 $10^{11}$ 到 $10^{18}$ 倍。消融实验中FFN 端到端偏差仅 $9.5 \times 10^{-7}$，验证了误差被限制在非线性计算单点。在大模型对比中，我们的方法在 LLaMA-2 7B 上困惑度升高为基线方法的0.05%，在70B参数量级端到端实验中，整体精度降幅小于 $2\%$，体现了结构一致性的可扩展优势。在神经形态硬件Loihi 2 上的能耗约为 GPU 的 $2\%$。
% Our experiments systematically validate the effectiveness of our method. Under the same number of time steps, our encoding yields errors about $10^{11}$ to $10^{18}$ times lower than the baseline. In ablation studies, the end-to-end deviation of the FFN is only $9.5\times10^{-7}$, confirming that error is confined to a single nonlinear computation point. In large-model comparisons, on LLaMA-2~7B the perplexity increase is $0.05\%$ of that of the baseline method; at the 70B scale in end-to-end experiments, the overall accuracy drop is below $2\%$, reflecting the scalability afforded by structural consistency. On neuromorphic hardware, energy consumption on Loihi~2 is about $2\%$ of that on the GPU.


% Nevertheless, existing approaches can be broadly divided into three categories, each facing both unique limitations and a common structural bottleneck.

% First, rate coding and other long-window schemes provide a straightforward conversion by accumulating spike counts, but they suffer from low information density, high latency, and poor efficiency~\citep{rueckauer2017conversion,bu2023ultralow,auge2021encoding,guo2021coding}. Time-to-first-spike (TTFS) and related single-spike schemes attempt to shorten windows, but in practice they require many timesteps to achieve acceptable precision, undermining the latency advantage~\citep{bonilla2022ttfs,stanojevic2024ttfs}. Beyond these specific flaws, such encoding strategies fundamentally lack a mechanism to localize approximation: errors arise from the stochasticity of spike counts or spike timing and diffuse along the temporal dimension as networks deepen, breaking structural consistency with the ANN forward path.

% Second, surrogate-based nonlinearities introduce differentiability by replacing non-differentiable spike functions with smooth substitutes~\citep{neftci2019surrogate,wu2018stbp,shrestha2018slayer}. While this enables gradient flow, it inevitably injects approximations at every activation site. The unique drawback here is that each layer becomes a new source of approximation error, so the accumulated mismatch grows with depth. At the same time, these methods also fail to localize errors: rather than being confined to one module, errors propagate layer by layer, making the overall network lose structural consistency with the original ANN design.

% Third, hybrid conversion–training pipelines combine approximate operator conversion with fine-tuning or multi-stage training~\citep{rathi2020hybrid,rathi2021dietsnn}. Their unique limitation is that multi-phase optimization introduces instability and often requires specialized architectures or long timesteps. More critically, such pipelines distribute approximations across multiple modules, so the forward and backward paths no longer share a consistent structure. In other words, they not only inherit instability from training tricks but also lack error localization and global structural consistency.

% In parallel, recent attempts have scaled SNNs to Transformer and LLM scenarios, demonstrating initial feasibility~\citep{zhou2023spikformer,zhu2023spikegpt,xing2024spikelm}. Yet these works still rely on long windows, high firing rates, or surrogate-style substitutes, so errors are introduced at multiple sites and diffuse through time and depth. Under short-timestep budgets and ultra-large-scale models, this lack of error localization and structural consistency makes training unstable and fidelity hard to preserve, limiting their deployability.


% Thus, we aim to answer a core question: is it possible to construct a high-fidelity structural mapping mechanism from ANNs to SNNs, so that as the model scales up, it can still maintain accuracy consistency, error controllability, and trainability? To this end, we propose a roadmap for gradually approaching an ideal SNN, with the core objective of maximizing the preservation of information expression capability, nonlinear modeling capability, and tunability, while controlling errors within predictable single-point regions of the network structure.

% To achieve this goal, our first step is to propose Bit Spike Encoding (BSE), whose significance lies not only in replacing the encoding form but also in endowing the network with a new spiking expression language. Traditional neurons represent states through activation values, while our BSE transforms them into sparse and ordered spike sequences in the time domain, compressing expression within a fixed time window, thereby reconstructing information density from the numerical space to the spiking space. The second step is to introduce Adaptive Spiking Neural Codec (ASNC) to implement nonlinear activation functions. Since activation functions are irreversible and difficult to precisely simulate, we design ASNC as a structural approximator. We use a piecewise adaptive strategy to fit nonlinear curves while ensuring that outputs remain in BSE encoding format, thus enabling end-to-end inference in the spiking space. Although unavoidable approximation errors are introduced here, we strictly confine them within the ASNC single-module, preventing propagation or accumulation within the network and forming a localized mechanism. On the basis of ensuring high fidelity in the forward path, we propose an STE-based approximate training scheme. Unlike previous approaches that introduced approximate non-differentiable methods at multiple structural points, we have already compressed SNN behavior into a form almost equivalent to ANNs in the first two steps, so that backpropagation only needs to approximate gradients within an extremely small error space. At this point, the introduced STE is no longer a rough substitute but a gradient stable backpropagation mechanism supported by functional consistency, thereby achieving both convergence and generalization guarantees~\citep{neftci2019surrogate}. The key of the entire framework lies in that we no longer attempt to make low-fidelity approximations at multiple positions, but instead carefully design the system so that errors are controlled at the only controllable node within the structure, while maintaining the interpretability, tunability, and deployability of the overall system.

% Within this overall framework, we systematically validated the effectiveness of our approach through extensive experiments. First, at the encoding level, BSE demonstrated orders-of-magnitude higher reconstruction fidelity compared to rate coding and TTFS under the same latency budget: for FP16, the mean squared error (MSE) was only \textbf{$1.03\times10^{-9}$}, and for FP32 it reached \textbf{$1.33\times10^{-14}$}, approaching machine precision. Second, at the non-linear stage, ASNC strictly confined approximation errors to a single local module, with an end-to-end error of only \textbf{$9.51\times10^{-7}$}, while conventional rate coding under the same condition produced an error as large as \textbf{$4.88\times10^{-1}$}, confirming our “single-point error controllability” hypothesis. Further ablation studies showed that replacing either the high-fidelity encoding or the non-linear approximation with low-fidelity alternatives caused catastrophic degradation (e.g., Rate Coding + ASNC yielded a PPL above \textbf{5000}), whereas using BSE+ASNC increased perplexity only slightly, from \textbf{$2.88\pm0.01$} to \textbf{$2.95\pm0.02$}, i.e., by about \textbf{0.07}. On large-scale models such as Llama-2 (70B), our high-fidelity ANN$\to$SNN mapping achieved less than \textbf{2\%} accuracy drop across benchmarks including MMLU, HellaSwag, ARC, and TruthfulQA~\citep{hendrycks2021mmlu,zellers2019hellaswag,clark2018arc,lin2022truthfulqa}, with the degradation further diminishing as model size increased—for example, the MMLU score of Llama-2 70B decreased by only \textbf{1.2\%}, indicating an emergent robustness of larger models. Finally, on Intel’s Loihi 2 neuromorphic hardware, the energy cost of a single ASNC non-linear operation was only about \textbf{0.5\%} of its SiLU counterpart on an NVIDIA A100 GPU, representing a two-order-of-magnitude reduction, while maintaining nearly identical model performance~\citep{orchard2021loihi2}. These results collectively confirm that our roadmap achieves the threefold goals of information fidelity, controllable non-linear approximation, and stable trainability in practice.
