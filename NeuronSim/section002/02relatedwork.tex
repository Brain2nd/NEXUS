% High-fidelity mapping and trainability from ANNs to SNNs have long been constrained by three fundamental obstacles: first, the equivalence between numerical activations and spike sequence representations is difficult to establish, and simple replacement leads to decreased information density and soaring latency; second, the spiking implementations of nonlinear numerical computations such as ReLU/Softmax/BN lack structural generalization capability, with errors often diffusing at multiple points within the network; third, temporal dependency and binary firing cause backpropagation to suffer from issues such as unstable training, diffusion of information loss, and uncontrollable errors ~\citep{neftci2019surrogate,wu2018stbp,shrestha2018slayer}.

% In the direct conversion from numerical to spiking representations, the traditional paths exhibit clear limitations. Conventional rate coding, although easy to use, has low information density, poor expressive efficiency, and high latency; for example, to approximate 0.625, it often requires accumulating spike counts over a long time window to form a reliable estimate. Such “high-latency approximations” have long been pointed out in surveys to amplify jitter and estimation variance and slow convergence ~\citep{auge2021encoding,guo2021coding}. Mainstream ANN→SNN conversion works, while systematizing equivalent implementations of operators such as threshold/weight normalization, Softmax, and BN, are still essentially dominated by rate/time-window estimation (and thus limited in latency and energy efficiency) ~\citep{rueckauer2017conversion,gao2023highaccuracy,bu2023ultralow}. Time-to-first-spike (TTFS) or first-spike coding can improve speed within short time windows, but accuracy and robustness are often constrained by the single-spike requirement and training stability ~\citep{bonilla2022ttfs,stanojevic2024ttfs}. To establish a stable, equivalent, and tunable spiking representation, we introduce BSE (Bit Spike Encoding), which establishes a one-to-one bijection between a deterministic N-step spike sequence and any N-bit numerical value, achieving entropy preservation and reversible decoding under ideal discrete, noiseless, and lossless conditions. This fundamentally avoids the many-to-one information loss and error diffusion of traditional approximate encodings, while its cost is that latency is linearly proportional to bit width, forming a fixed and predictable delay budget (e.g., FP16 requires 16 time steps).

% Constructing nonlinear transformations in the spiking domain is highly challenging, and nonlinear operators provide networks with representational power and separability, so they cannot be skipped. In our setting, since the introduction of BSE makes the inputs/outputs of each layer spike sequences carrying temporal dimensions, the original ANN nonlinearities (such as SiLU/Softmax) can no longer be directly used, and precise and controllable nonlinear implementations must be given in the spiking domain. Existing paths either rely on delay/time-window “simulation-style” approximations, or tailor “spike-friendly” structures for local components, such as the Spikformer series using Spiking Self-Attention (SSA) to realize Q/K/V in spiking form and avoid softmax ~\citep{zhou2023spikformer}, or through structural rewrites to circumvent non-spiking operators in attention/residual modules ~\citep{yao2023sdt}. Other works provide direct spiking equivalents of softmax/LayerNorm at the conversion level to support Transformer conversion, but overall remain limited by approximation errors and generalization ~\citep{you2024spikeziptf,spikedattention2024,tang2024sorbet}. Based on this, we propose a pluggable structural approximator, ASNC, to provide high-fidelity and controllable approximation, and to strictly confine errors within the nonlinear unit, avoiding their diffusion through the network.

% In terms of training, SNNs are usually constrained by the problems of non-differentiable gradients and discontinuous information. Although frameworks such as STBP, SLAYER, DIET-SNN, and hybrid conversion + fine-tuning have been developed, many approaches require special structures, long time windows, or multi-stage processes, which in practice introduce training instability and uncontrollable errors ~\citep{wu2018stbp,shrestha2018slayer,rathi2021dietsnn,rathi2020hybrid,neftci2019surrogate}. Given that our designed forward path already possesses high consistency, we can introduce lightweight STE substitutions on this basis for stability optimization. This aligns with the theoretical and empirical consensus of surrogate gradients/STE and is consistent in engineering goals with low-timestep-oriented training methods such as TET ~\citep{deng2022tet}.

% In recent years, a series of explorations of large SNN models have achieved initial progress on Transformer/LLM tasks and shown certain potential, but these methods mostly rely on long time windows/high firing rates or surrogate gradients, approximately differentiable activations, and other “soft substitutes” to obtain differentiability. They do not confine errors strictly to single points (such as nonlinear units), and errors easily diffuse along temporal and depth dimensions, causing unstable training. Thus, under short-timestep budgets and ultra-large-scale modeling, it is difficult to simultaneously maintain strong structural consistency with the original ANN and preserve high-fidelity accuracy, limiting engineering deployment. For example: Spikformer/Spikformer-v2 replace standard attention with SSA or softmax-free attention ~\citep{zhou2023spikformer,zhou2024spikformerv2}; SpikeGPT/Spiking-LLM use soft gating, approximately differentiable activations, and gated spiking units for autoregression ~\citep{zhu2023spikegpt,xing2025spikellm}; Spike-Driven Transformer and Spiked-Attention/Spiking Self-Attention perform spike-friendly structural rewrites in Q/K/V and normalization paths ~\citep{yao2023sdt,spikedattention2024,wang2023stsa}; SpikeZIP/SpikeZIP-TF align with ANNs through long time windows and reparameterization or quantization-equivalence, requiring spike-friendly activations for training/conversion alignment ~\citep{you2024spikeziptf}; SpikeLM explores direct training from scratch ~\citep{xing2024spikelm}. To address these challenges, we propose a roadmap for gradually approaching an ideal SNN, with the core objective of maximizing the preservation of information expression capability, nonlinear modeling capability, and tunability, while controlling errors within predictable single-point regions of the network structure.

\section{Related Works}
\label{sec:related_works}

% 从 ANN 到 SNN 的高保真映射与可训练性长期受制于三个核心障碍：其一，数值激活与脉冲序列之间的等价关系难以建立，简单替换会导致信息密度下降与延迟飙升；其二，诸如 ReLU、Softmax、BN 等非线性函数的脉冲实现缺乏结构泛化性，需要更换为脉冲友好的非线性计算并在替换后训练对齐；其三，时间依赖与二值发放使反向传播在训练中出现不稳定、信息损失扩散和误差不可控 ~\citep{neftci2019surrogate,wu2018stbp,shrestha2018slayer}。因此，需要一种结构化路线图，将误差限制在可预测的位置，同时保持与 ANN 的结构一致性。
% High-fidelity mapping and trainability from ANNs to SNNs have long been constrained by three fundamental obstacles: first, establishing the equivalence between numerical activations and spike sequences is difficult, and naive replacement leads to reduced information density and soaring latency; second, spiking implementations of nonlinear functions such as ReLU, Softmax, and BN lack structural generalization, often injecting errors at multiple points in the network; third, temporal dependency and binary firing cause backpropagation to suffer from instability, diffusion of information loss, and uncontrollable errors ~\citep{neftci2019surrogate,wu2018stbp}. Therefore, a structured roadmap is needed to confine errors to predictable loci while maintaining structural consistency with ANNs.

% 每一点加小标题

% \noindent\textbf{线性编码方式：}现有脉冲编码方式本质上均使用脉冲序列近似连续值进行信息表证，但误差大、延迟高。例如使用速率编码~\citep{rueckauer2017conversion}，需要使用1000时间步来实现对$0.625$的近似，信息密度低、表达效率差，会放大抖动并减缓收敛~\citep{auge2021encoding,guo2021coding}，导致难以在大规模建模场景上应用。近期的研究尝试使用效率来换取准确率。TTFS-Former 编码在精度上近似ANN~\citep{ZhaoHuangDingYu2025_TTFSFormer}，但由于单脉冲约束，仍需 1024 步才能达到 FP16 精度~\citep{stanojevic2024ttfs}。Spikformer/Spikformer-v2~\citep{zhou2023spikformer,zhou2024spikformerv2} 和 SpikeZIP/SpikeZIP-TF~\citep{you2024spikeziptf} 等方法也依赖长时间窗或高发放率来维持精度。而我们并非降低这种近似带来的损失，而是提出了一种不牺牲效率的脉冲序列与连续值的双向精确映射编码——Bit Spike Encoding(BSE)，使得脉冲序列能够在固定的低时间窗内精确表征连续值，从而消除误差在网络线性计算中的传播路径。

\noindent\textbf{Linear encoding.} Existing spike encoding methods essentially use spike sequences to approximate continuous values for information representation, but they suffer from large errors and high latency. For example, using rate coding~\citep{rueckauer2017conversion}, 1{,}000 timesteps are required to approximate $0.625$, which leads to low information density and poor expressive efficiency, amplifies jitter, and slows convergence~\citep{auge2021encoding,guo2021coding}, making it difficult to apply in large-scale modeling scenarios. Recent studies attempt to trade efficiency for accuracy. TTFS-Former encoding achieves accuracy close to ANNs~\citep{ZhaoHuangDingYu2025_TTFSFormer}, but due to the single-spike constraint, it still requires 1{,}024 steps to reach FP16 precision~\citep{stanojevic2024ttfs}. Methods such as Spikformer/Spikformer-v2~\citep{zhou2023spikformer,zhou2024spikformerv2} and SpikeZIP/SpikeZIP-TF~\citep{you2024spikeziptf} also rely on long time windows or high firing rates to maintain accuracy. Rather than reducing the losses caused by such approximations, we propose Bit Spike Encoding (BSE), a bidirectional exact mapping between spike sequences and continuous values, which enables spike sequences to precisely represent continuous values within a fixed short time window, thereby eliminating the error propagation path in network linear computations.  

% \noindent\textbf{非线性计算：}\noindent\textbf{非线性计算：}因为现有方法在处理非线性计算时，要么为某种线性计算设计专用近似模块，要么将部分模块转化为“spike-friendly”的替代结构并进行额外训练对齐，从而不可避免地引入影响后续计算的误差。Spiking Self-Attention (SSA) ~\citep{zhou2023spikformer}，通过结构改写绕过非脉冲算子 ~\citep{yao2023sdt,spikedattention2024,wang2023stsa}。也有一些工作直接给出 Softmax、LayerNorm 的脉冲等效形式 ~\citep{you2024spikeziptf,tang2024sorbet}，以支持 Transformer 转换，SpikeGPT ~\citep{zhu2023spikegpt} 与 Spiking-LLM ~\citep{xing2025spikellm} 采用软门控与近似可微单元来实现建模。Rather than 设计专用近似结构或替代，我们提出通用结构化逼近器 Adaptive Spiking Neural Codec (ASNC)，分段拟合任意非线性计算，且输入输出保持BSE格式，从而使误差的累计扩散途径被后续线性计算截断，形成误差局部化机制。

\noindent\textbf{Nonlinear computation.} Because existing methods in handling nonlinear computations either design dedicated approximation modules for certain linear operations, or convert some modules into ``spike-friendly'' substitute structures with additional training alignment, they inevitably introduce errors that affect subsequent computations. Spiking Self-Attention (SSA)~\citep{zhou2023spikformer} bypasses non-spike operators through structural rewriting~\citep{yao2023sdt,spikedattention2024,wang2023stsa}. Some works also directly provide spike-domain equivalent forms of Softmax and LayerNorm~\citep{you2024spikeziptf,tang2024sorbet} to support Transformer conversion, while SpikeGPT~\citep{zhu2023spikegpt} and Spiking-LLM~\citep{xing2025spikellm} adopt soft gating and approximately differentiable units for modeling. Rather than designing specialized approximations or substitutes, we propose a general structural approximator, Adaptive Spiking Neural Codec (ASNC), which performs piecewise approximation for arbitrary nonlinear computations while keeping inputs and outputs in the BSE format, so that the potential path of error accumulation and diffusion is cut off by subsequent linear computations, forming a localized error mechanism.  

% \noindent\textbf{训练：}SNN 的梯度计算受到脉冲不可微的限制，对不可导函数进行梯度替代会引入额外误差，影响训练收敛。SpikeLM ~\citep{xing2024spikelm} 尝试直接训练，依赖 surrogate 或软替代保证可微性，不可避免地引入误差。STBP ~\citep{wu2018stbp}、SLAYER ~\citep{shrestha2018slayer}、DIET-SNN ~\citep{rathi2021dietsnn} 以及转换+微调 ~\citep{rathi2020hybrid}，往往需要多阶段流程或特殊结构，并在多个层次引入 surrogate gradient，降低训练稳定性。我们的方案则在 BSE 与 ASNC 保证的高保真、结构一致前向路径基础上，引入轻量的 Straight-Through Estimator (STE)。虽然在全局范围内使用，但由于误差被严格固定在唯一的非线性模块，且梯度路径与数值路径完全对齐，反向传播不会额外改变计算过程，从而保证了训练的收敛稳定。

\noindent\textbf{Training.} The gradient computation of SNNs is limited by the non-differentiability of spikes, and replacing non-differentiable functions with surrogate gradients introduces additional errors that affect training convergence. SpikeLM~\citep{xing2024spikelm} attempts direct training, relying on surrogates or soft substitutes to ensure differentiability, but inevitably introduces errors. STBP~\citep{wu2018stbp}, SLAYER~\citep{shrestha2018slayer}, DIET-SNN~\citep{rathi2021dietsnn}, and conversion with fine-tuning~\citep{rathi2020hybrid} often require multi-stage procedures or special structures, and introduce surrogate gradients at multiple levels, reducing training stability. Our approach instead introduces a lightweight Straight-Through Estimator (STE) on top of the high-fidelity and structurally consistent forward path guaranteed by BSE and ASNC. Although used globally, since errors are strictly confined to the single nonlinear module and the gradient path is fully aligned with the numerical path, backpropagation does not introduce additional modifications to the computation process, thereby ensuring stable training convergence.



% % 速率编码实现简单~\citep{rueckauer2017conversion}，但信息密度低、表达效率差，需要较长的时间窗。例如，为了近似 $0.625$，常常需要超过 1000 个脉冲来计算频率，这会放大抖动并减缓收敛~\citep{auge2021encoding,guo2021coding}。主流的 ANN$\to$SNN 转换方法提出了阈值归一化、Softmax 和 BN 的等效实现~\citep{rueckauer2017conversion,gao2023highaccuracy,bu2023ultralow}，但仍然受限于速率/时间窗估计。在大规模建模中，Spikformer/Spikformer-v2~\citep{zhou2023spikformer,zhou2024spikformerv2} 和 SpikeZIP/SpikeZIP-TF~\citep{you2024spikeziptf} 等方法依赖长时间窗或高发放率来维持精度，但其表示仍然是近似的，误差会随时间扩散。TTFS-Former 编码在精度上更高~\citep{ZhaoHuangDingYu2025_TTFSFormer}，但由于单脉冲约束，仍需 1024 步才能达到 FP16 精度~\citep{stanojevic2024ttfs}。相比之下，我们提出的 Bit Spike Encoding (BSE) 建立了确定性的 $N$ 步 $\leftrightarrow$ $N$ 位数值双射（例如 FP16 只需 16 步），在理想的离散、无噪声、无损条件下能够保持熵不变并支持可逆解码。这避免了近似编码的多对一信息丢失与误差扩散，同时保持延迟成本固定且可预测。

% In terms of encoding, rate coding is simple to implement ~\citep{rueckauer2017conversion}, but its information density is low, expressive efficiency is poor, and long time windows are required. For example, to approximate $0.625$, more than 1000 spikes are often needed to compute frequency, which amplifies jitter and slows convergence ~\citep{auge2021encoding,guo2021coding}. Mainstream ANN$\to$SNN conversion methods propose equivalent implementations for threshold normalization, Softmax, and BN ~\citep{rueckauer2017conversion,gao2023highaccuracy,bu2023ultralow}, but they remain fundamentally constrained by rate/time-window estimation. In large-scale modeling, approaches such as Spikformer/Spikformer-v2 ~\citep{zhou2023spikformer,zhou2024spikformerv2} and SpikeZIP/SpikeZIP-TF ~\citep{you2024spikeziptf} rely on long time windows or high firing rates to maintain accuracy, but their representations remain approximate and errors diffuse over time. TTFS-Former coding achieves higher precision ~\citep{ZhaoHuangDingYu2025_TTFSFormer}, yet still requires 1024 steps to match FP16 precision due to the single-spike constraint ~\citep{stanojevic2024ttfs}. By contrast, our Bit Spike Encoding (BSE) establishes a deterministic $N$-step $\leftrightarrow$ $N$-bit numerical bijection (e.g., FP16 requires 16 steps), which preserves entropy and supports reversible decoding under ideal discrete, noiseless, and lossless conditions. This avoids the many-to-one information loss and error diffusion of approximate encodings, while keeping the latency cost fixed and predictable.

% % 非线性计算：已有方法通常依赖延迟或长时间窗的模拟近似 ~\citep{rueckauer2017conversion}，或者为部分模块设计“脉冲友好”替代结构，例如 Spiking Self-Attention (SSA) ~\citep{zhou2023spikformer}，或通过结构改写绕过非脉冲算子 ~\citep{yao2023sdt,spikedattention2024,wang2023stsa}。也有一些工作直接给出 Softmax、LayerNorm 的脉冲等效形式 ~\citep{you2024spikeziptf,tang2024sorbet}，以支持 Transformer 转换。在大规模建模中，SpikeGPT ~\citep{zhu2023spikegpt} 与 Spiking-LLM ~\citep{xing2025spikellm} 采用软门控与近似可微单元来实现自回归建模，但不可避免在网络多个位置注入近似误差并扩散。相比之下，我们提出的 Adaptive Spiking Neural Codec (ASNC) 作为可插拔的结构化逼近器，可以对任意非线性函数进行分段拟合，输出保持 BSE 格式，从而将误差严格限制在唯一的非线性模块中，避免在网络中多点扩散。
% In terms of nonlinear construction, existing methods often rely on delay- or time-window-based approximations ~\citep{rueckauer2017conversion}, or design ``spike-friendly'' replacements for local components, such as Spiking Self-Attention (SSA) ~\citep{zhou2023spikformer}, or structural rewrites to bypass non-spiking operators ~\citep{yao2023sdt,spikedattention2024,wang2023stsa}. Some works also provide direct spiking equivalents for Softmax and LayerNorm ~\citep{you2024spikeziptf,tang2024sorbet} to support Transformer conversion. In large-scale settings, SpikeGPT ~\citep{zhu2023spikegpt} and Spiking-LLM ~\citep{xing2025spikellm} adopt soft gating and approximately differentiable units to enable autoregressive modeling, but inevitably inject approximation errors at multiple points in the network, which then diffuse. By contrast, our Adaptive Spiking Neural Codec (ASNC) serves as a pluggable structural approximator, fitting arbitrary nonlinearities in piecewise form while ensuring outputs remain in BSE format. This strictly confines approximation error to a single nonlinear module and prevents multi-point error diffusion across the network.

% % 训练：SNN 的梯度计算受到脉冲不可微的限制。已有方法如 STBP ~\citep{wu2018stbp}、SLAYER ~\citep{shrestha2018slayer}、DIET-SNN ~\citep{rathi2021dietsnn} 以及转换+微调 ~\citep{rathi2020hybrid}，往往需要多阶段流程或特殊结构，并在多个层次引入 surrogate gradient，降低训练稳定性。在大规模模型探索中，SpikeLM ~\citep{xing2024spikelm} 尝试直接训练，但仍需依赖 surrogate 或软替代保证可微性，从而在多个层次注入误差。我们的方案则在 BSE 与 ASNC 保证的高保真、结构一致前向路径基础上，引入轻量的 Straight-Through Estimator (STE)。虽然在全局范围内使用，但由于误差被严格固定在唯一的非线性模块，STE 的近似不会扩散，从而实现稳定的收敛，这与量化感知训练的理论以及低时间步方法如 TET ~\citep{deng2022tet} 的经验一致。
% In terms of training, SNN gradient computation is fundamentally limited by the non-differentiability of spikes. Existing methods such as STBP ~\citep{wu2018stbp}, SLAYER ~\citep{shrestha2018slayer}, DIET-SNN ~\citep{rathi2021dietsnn}, and hybrid conversion + fine-tuning ~\citep{rathi2020hybrid} often require multi-stage processes or specialized structures, introducing surrogate gradients at multiple layers and thereby reducing stability. In large-scale model training, SpikeLM ~\citep{xing2024spikelm} attempts direct training but still relies on surrogates or soft substitutes to enforce differentiability, which inject errors at multiple points. In contrast, our method introduces a lightweight Straight-Through Estimator (STE) on top of the high-fidelity, structurally consistent forward path guaranteed by BSE and ASNC. Although applied globally, errors are strictly confined to the unique nonlinear module, ensuring that gradient approximations do not diffuse. This enables stable convergence and aligns with both the theory of quantization-aware training and the empirical evidence of low-timestep methods such as TET ~\citep{deng2022tet}.

% % % 综合来看，现有的大规模 SNN 方法往往依赖长时间窗、高发放率或多点近似，误差难以控制并在深度与时间维度扩散，因而难以在短时间窗预算与超大规模条件下保持高保真精度。与此对比，LASER 通过“BSE 等价表达—ASNC 单点近似—STE 稳定训练”的三步路线，在 70B 规模模型上依然保持超过 98\% 的 ANN 性能，并在 Loihi~2 上实现约 $200\times$ 的能效提升，证明了高保真、误差可控与可扩展性可以在大规模脉冲模型中统一实现。
% % Overall, existing large-scale SNN approaches often rely on long time windows, high firing rates, or multi-point approximations, leading to uncontrolled errors that diffuse along temporal and depth dimensions, making it difficult to maintain high-fidelity accuracy under short timestep budgets and ultra-large-scale conditions. By contrast, LASER follows a three-step roadmap of ``BSE equivalence, ASNC single-point approximation, and STE-stabilized training,'' achieving over 98\% of ANN performance even on 70B-scale models, and delivering approximately $200\times$ energy efficiency gains on Loihi~2, thereby demonstrating that fidelity, controllability, and scalability can be unified in large-scale spiking models.
