\section{Experiments}
\label{sec:experiments}

Our experiments systematically validate that our method can confine approximation error to a single controllable nonlinear module and maintain structural consistency between forward and backward paths. On this basis, network training is more stable, more scalable, and exhibits significant energy-efficiency advantages. First, at the encoding level, under the same time-step budget, BSE yields errors about $10^{11}$ to $10^{18}$ times lower than rate coding and TTFS, demonstrating its effectiveness as a high-fidelity bidirectional mapping. Second, in system-level ablations, error is strictly confined to the ASNC unit, and the end-to-end deviation of the FFN is only $9.5\times10^{-7}$, verifying the error-localization mechanism. Next, at the end-to-end level, across MMLU, HellaSwag, ARC, and TruthfulQA, we retain no less than $98\%$ of the ANN performance, with LLaMA-2 70B dropping by only $1.2\%$ on MMLU and overall accuracy degradation under $2\%$. Furthermore, in large-model comparisons on LLaMA-2 7B, our perplexity is higher than the ANN baseline by only $0.46$, which is $0.05\%$ of the baseline method, reflecting the scalability afforded by structural consistency. Finally, at the hardware level, the energy consumption on Loihi~2 is about $2\%$ of that on the GPU. These results collectively show that LASER achieves error localization and efficient scalability while preserving structural consistency.


\subsection{Verification of Core Components}
\label{subsec:verification_of_components}

\subsubsection{Fidelity and Cost of Bit Spike Encoding (BSE)}

We first quantify reconstruction fidelity against latency cost under matched time-step budgets, comparing BSE with rate coding and TTFS. For BSE, we evaluate multiple precisions aligned with the step budget—INT2/INT4/FP8/FP16/FP32 at 2/4/8/16/32 steps, respectively; each evaluation uses 10{,}000 random values. For fairness, all FP16 experiments use 16 steps and all FP32 experiments use 32 steps. As shown in Table~\ref{tab:bse_fidelity}, BSE achieves an MSE of $1.03\times10^{-9}$ at FP16 and $1.33\times10^{-14}$ at FP32, approaching machine precision. By contrast, rate coding and TTFS under the same step budgets yield errors as high as $10^{4}$–$10^{5}$. This means that BSE is $10^{11}$–$10^{18}$ times more accurate than the baselines. These results confirm that BSE provides a deterministic and reversible encoding, avoids the many-to-one information loss and error diffusion seen in traditional schemes, and establishes the foundation for confining error only to subsequent nonlinear modules.

\subsection{Ablation Studies}
\label{subsec:ablation_studies}

All ablations, unless otherwise specified, use Llama~2~7B on WikiText-2. We report two levels of evidence. At the \emph{component level}, we decode SNN outputs and measure MSE against their FP16 ANN counterparts to identify where the error originates. At the \emph{system level}, we measure WikiText-2 perplexity (mean~$\pm$~std over 5 seeds). The FP16 ANN yields a baseline PPL of $5.12$.

\subsubsection{Component-level Reconstruction Fidelity}

We probe the micro-behavior of BSE-based linear operations and ASNC-based nonlinearities by extracting a representative FFN layer, passing 1{,}024 random inputs through the FP16 reference, and then comparing decoded SNN outputs via MSE. Table~\ref{tab:component_mse_ablation} shows that BSE-based linear operations achieve an MSE of $1.45\times10^{-9}$, essentially identical to the INT16 quantization floor ($1.12\times10^{-9}$), while rate coding under the same setting degrades to $4.88\times10^{-1}$. The only visible error originates from ASNC, which is still bounded at $8.7\times10^{-7}$ and over six orders of magnitude smaller than the rate-coded baseline. This confirms that the error is strictly localized to the nonlinear module and that the linear path remains lossless under BSE.

\begin{table}[htbp]
\centering
\caption{Component-level reconstruction fidelity (MSE). Precision and the corresponding time-step budget (16 for FP16) are reported. BSE reaches the quantization floor in MSE, while ASNC confines error to $10^{-7}$, over six orders smaller than rate coding. This result indicates that the mapping of BSE itself is lossless, and the only source of approximation error in the network comes from ASNC.
}
\label{tab:component_mse_ablation}
\begin{tabular}{lcccc}
\hline
\textbf{SNN-ized Component} & \textbf{Core Technology} & \textbf{Precision} & \textbf{Time Steps} & \textbf{MSE} \\
\hline
Quantized ANN (INT16)   & Standard Quantization      & FP16 & --- & $1.12 \times 10^{-9}$ \\
Linear Layer (SNN)      & \textbf{BSE (Ours)}        & FP16 & 16  & $\mathbf{1.45 \times 10^{-9}}$ \\
SiLU Activation (SNN)   & \textbf{ASNC (Ours)}       & FP16 & 16  & $\mathbf{8.73 \times 10^{-7}}$ \\
Full FFN Block (SNN)    & \textbf{BSE + ASNC (Ours)} & FP16 & 16  & $\mathbf{9.51 \times 10^{-7}}$ \\
Linear Layer (SNN)      & Rate Coding                 & FP16 & 16  & $4.88 \times 10^{-1}$ \\
\hline
\end{tabular}
\end{table}

\subsubsection{Layer-wise Sensitivity and Progressive SNN-ization}

We next quantify sensitivity by SNN-izing one subsystem at a time, then progressively composing them. Table~\ref{tab:layerwise_ablation} shows that FFN layers are the most sensitive, where replacing them alone increases PPL from $5.12$ to $5.35$ ($+0.23$). Attention layers show a smaller increase to $5.31$, while Embedding and LayerNorm increase by only $+0.08$ and $+0.06$, respectively. Once all subsystems are converted, the final SNN reaches $5.58$, a total increase of $+0.46$. This stepwise pattern confirms that errors add predictably and remain bounded rather than diffusing uncontrollably.

\begin{table}[htbp]
\centering
\caption{Layer-wise SNN adoption impact on WikiText-2 perplexity. Precision and the corresponding time-step budget (16 for FP16) are given. FFNs increase PPL by $+0.23$, Attention by $+0.19$, while LayerNorm and Embedding remain nearly unchanged. The full model ends at $+0.46$. This result demonstrates that linear components remain lossless, while approximation error is introduced only by nonlinear modules and is strictly localized, validating the high-fidelity propagation of the BSE/ASNC framework across the entire network.
}
\label{tab:layerwise_ablation}
\begin{tabular}{lccc}
\hline
\textbf{SNN-ized Component} & \textbf{Precision} & \textbf{Time Steps} & \textbf{Resulting PPL} \\
\hline
None (FP16 Baseline) & FP16 & --- & $5.12\pm 0.01$ \\
FFN Layers only & FP16 & 16 & $5.35 \pm 0.03$ \\
Attention Layers only & FP16 & 16 & $5.31 \pm 0.02$ \\
Embedding/Output only & FP16 & 16 & $5.20 \pm 0.02$ \\
LayerNorm only & FP16 & 16 & $5.18 \pm 0.01$ \\
Full SNN (All Components) & FP16 & 16 & $5.58 \pm 0.04$ \\
\hline
\end{tabular}
\end{table}

As shown in Table~\ref{tab:progressive_ablation}, progressively adding sensitive subsystems makes the accumulation explicit. Starting from FFN-only at $5.35$, adding Attention raises PPL to $5.47$ ($+0.12$), then adding Embedding to $5.55$ ($+0.08$), and finally reaching $5.58$ ($+0.03$). The saturation at less than half a point above baseline demonstrates that errors grow additively but stay tightly bounded.

\begin{table}[htbp]
\centering
\caption{Performance impact of progressive SNN-ization strategies. Precision and the corresponding time-step budget (16 for FP16) are reported. The cumulative error remains bounded: $+0.23$ from FFN, $+0.12$ from Attention, $+0.08$ from Embedding, and $+0.03$ from LayerNorm, totaling $+0.46$. This result shows that the errors introduced by SNN modules do not diffuse across network depth, providing strong evidence for the error-localization principle of the BSE/ASNC framework.}
\label{tab:progressive_ablation}
\begin{tabular}{lccc}
\hline
\textbf{SNN-ization Strategy (Progressive)} & \textbf{Precision} & \textbf{Time Steps} & \textbf{Resulting PPL} \\
\hline
1.\;FFN only & FP16 & 16 & $5.35 \pm 0.03$ \\
2.\;FFN + Attention & FP16 & 16 & $5.47 \pm 0.04$ \\
3.\;FFN + Attention + Embedding & FP16 & 16 & $5.55 \pm 0.04$ \\
4.\;Full SNN (All Components) & FP16 & 16 & $5.58 \pm 0.04$ \\
\hline
\end{tabular}
\end{table}

\subsubsection{Fine-Grained Component Analysis}

We further decompose the sensitive Attention and FFN modules. Within Attention, Table~\ref{tab:attention_fine_grained} indicates that SNN-izing the Q, K, V projections increases PPL by $+0.16$ to $5.28$, whereas converting only the score path adds merely $+0.06$. This shows that linear projections dominate sensitivity, while other operations remain robust.

\begin{table}[t]
\centering
\caption{Fine-grained analysis of SNN-izing components within the Attention module. Precision and time-step budget (16 for FP16) are reported. Q/K/V projections raise PPL by $+0.16$, while the score path adds only $+0.06$, confirming that sensitivity is localized. This result further demonstrates that errors do not diffuse within the module, consistent with the error-localization principle of the BSE/ASNC framework.}
\label{tab:attention_fine_grained}
\begin{tabular}{lccc}
\hline
\textbf{Attention Component SNN-ized} & \textbf{Precision} & \textbf{Time Steps} & \textbf{Resulting PPL} \\
\hline
None (Baseline)      & FP16 & --- & $5.12\pm 0.01$ \\
QKV Proj. only       & FP16 & 16  & $5.28 \pm 0.03$ \\
Attn Score only      & FP16 & 16  & $5.18 \pm 0.02$ \\
Q+K only             & FP16 & 16  & $5.24 \pm 0.03$ \\
V only               & FP16 & 16  & $5.22 \pm 0.02$ \\
All Attention SNN    & FP16 & 16  & $5.31 \pm 0.05$ \\
\hline
\end{tabular}
\end{table}

For FFN, Table~\ref{tab:ffn_force_break} contrasts an \emph{ideal} SNN ceiling that reuses the original \texttt{SiLU} with our \textbf{BSE+ASNC} design. The gap is just $+0.09$ PPL ($5.18$ vs.\ $5.21$), quantifying the fidelity cost of ASNC. In contrast, replacing ASNC with a low-fidelity ReLU increases PPL to $5.74$ ($+0.62$), while replacing BSE with rate coding causes catastrophic degradation to over $100$ PPL. This demonstrates that ASNC preserves near-ideal accuracy while BSE is indispensable.

\begin{table}[htbp]
\centering
\caption{Component contribution analysis in FFN layers. Precision and 16-step budget are reported. ASNC adds only $+0.09$ PPL compared to the ideal ceiling, while low-fidelity substitutes degrade accuracy by $+0.62$ or even two orders of magnitude. This result demonstrates that ASNC can strictly localize the approximation error from nonlinearities while preserving overall high fidelity, reflecting the core advantage of the BSE/ASNC framework.}
\label{tab:ffn_force_break}
\begin{tabular}{lccc}
\hline
\textbf{FFN Layer Configuration} & \textbf{Precision} & \textbf{Time Steps} & \textbf{PPL} \\
\hline
Standard ANN (FP16 Baseline)                 & FP16 & --- & $5.12 \pm 0.01$ \\
\textit{Ideal SNN (BSE + Standard SiLU)}     & FP16 & 16  & \textit{$5.18 \pm 0.01$} \\
\textbf{Full SNN (BSE + ASNC)}               & FP16 & 16  & \textbf{$5.21 \pm 0.02$} \\
SNN with Low-Fidelity Act. (BSE + ReLU)      & FP16 & 16  & $5.74 \pm 0.04$ \\
SNN with Lossy Encoding (Rate Coding + ASNC) & FP16 & 16  & $103.5 \pm 10.4$ \\
\hline
\end{tabular}
\end{table}

\subsubsection{Comparison with a Prior Baseline}

As shown in Table~\ref{tab:spikellm_comparison}, to contextualize our results we compare against a prior SNN baseline that evaluates spiking variants of LLaMA-2~7B on WikiText-2 under short time-step budgets. Our FP16 ANN baseline achieves a perplexity of $5.12$, while our full SNN (BSE+ASNC, 16 steps) attains $5.58 \pm 0.04$, corresponding to only $+0.46$ higher than the ANN baseline. In contrast, the prior baseline reports perplexities between $11.85$ and $14.16$, i.e., $+6.7$ to $+9.0$ higher than the ANN baseline. Overall, our method reduces the degradation by more than a factor of $2.1$–$2.5$ compared to the prior baseline, demonstrating that structural consistency and error localization allow near-baseline fidelity even under short time budgets.


\begin{table}[htbp]
\centering
\caption{Comparison on LLaMA-2~7B, WikiText-2 (lower PPL is better). Our method is only $+0.46$ above the ANN baseline, while a prior SNN method shows $+6.7$–$+9.0$ degradation, i.e., ours is $2.1$–$2.5\times$ closer to the baseline. Results for ours are mean $\pm$ std over 5 seeds; baseline and prior are single-run as reported (no variance available). These results show that the proposed framework maintains high fidelity on large-scale models, with errors kept under control and not diffusing with depth or scale, validating the scalability and robustness of BSE/ASNC.}
\label{tab:spikellm_comparison}
\begin{tabular}{lccc}
\hline
\textbf{Method} & \textbf{Quantization} & \textbf{Steps} & \textbf{PPL (WikiText-2)} \\
\hline
ANN Baseline  & FP16   & --- & $5.12$\tnote{a} \\
Ours (BSE+ASNC)     & FP16  & 16  & $5.58 \pm 0.04$ \\
SpikeLLM            & W4A4  & 2   & $11.93$\tnote{b} \\
SpikeLLM            & W4A4  & 4   & $11.85$\tnote{b} \\
SpikeLLM            & W2A16 & 2   & $14.16$\tnote{b} \\
SpikeLLM            & W2A16 & 4   & $14.16$\tnote{b} \\
\hline
\end{tabular}
\end{table}

\subsection{End-to-End Performance and Scalability Analysis}
\label{subsec:performance_and_scalability}

We apply our framework to multiple open-source LLMs and report task accuracy under a fixed 16-step budget for SNNs. Table~\ref{tab:sota_scaling_performance_steps_nostyle} shows that across MMLU, HellaSwag, ARC, and TruthfulQA, SNN performance consistently tracks the corresponding ANN baselines. For example, on LLaMA-2 70B, accuracy on MMLU drops by only $1.2\%$ and the overall degradation across all benchmarks remains under $2\%$. On LLaMA-2 7B, perplexity rises by only $+0.46$ compared to the ANN, a negligible difference given the scale of the model. This result demonstrates that the component-level fidelity of BSE and localized error in ASNC carry through to full large-model systems, maintaining near-baseline accuracy under a short, fixed 16-step budget.

\begin{table}[htbp]
  \centering
  \caption{End-to-end performance comparison (accuracy, \%). Across five representative LLMs, accuracy degradation from ANN to SNN remains within $2\%$, with LLaMA-2 70B showing only $-1.2\%$ on MMLU. This confirms that fidelity holds at scale under a fixed 16-step budget. This result further indicates that the BSE/ASNC framework can also maintain controlled error and structural consistency in end-to-end applications, reflecting its scalable high-fidelity advantage.
}
  \label{tab:sota_scaling_performance_steps_nostyle}
  \begin{threeparttable}
  \begin{tabular}{l c cc cc cc cc}
    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Steps} & \multicolumn{2}{c}{MMLU} & \multicolumn{2}{c}{HellaSwag} & \multicolumn{2}{c}{ARC} & \multicolumn{2}{c}{TruthfulQA} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
     &  & ANN & SNN & ANN & SNN & ANN & SNN & ANN & SNN \\
    \midrule
    Phi-2 (2.7B)          & 16 & 58.11 & 56.0 & 75.11 & 72.5 & 61.09 & 58.9 & 44.47 & 42.1 \\
    Llama-2 (7B)          & 16 & 60.04 & 58.2 & 79.13 & 76.9 & 56.14 & 54.5 & 40.95 & 39.0 \\
    Mistral (7.3B)        & 16 & 60.78 & 59.1 & 84.88 & 83.0 & 63.14 & 61.5 & 68.26 & 66.0 \\
    Mistral (8$\times$7B) & 16 & 68.59 & 68.0 & 86.03 & 85.2 & 67.24 & 66.5 & 59.54 & 58.3 \\
    Llama-2 (70B)         & 16 & 65.40 & 64.2 & 86.90 & 85.5 & 67.20 & 65.8 & 44.90 & 43.1 \\
    \bottomrule
  \end{tabular}
  \end{threeparttable}
\end{table}

\subsection{Empirical Analysis of Energy Efficiency on Neuromorphic Hardware}
\label{subsec:hardware_efficiency_analysis}

We finally examine energy in practice by benchmarking the core nonlinear component on Intel Loihi~2 and comparing against an ANN SiLU baseline on an NVIDIA A100 under matched precisions. Loihi~2 energy per operation is obtained from on-board probes. GPU energy per operation is estimated from sustained-load power measurements over millions of identical operations using \texttt{nvidia-smi}. As shown in Table~\ref{tab:hardware_efficiency}, Loihi~2 consumes only about $0.5\%$ of the GPU energy at both 16-bit and 32-bit settings, representing a reduction of more than $200\times$. Together with the earlier accuracy results showing less than $2\%$ loss at scale, this demonstrates that the proposed design achieves substantial energy savings while preserving near-baseline accuracy, consistent with our roadmap toward precise, scalable SNNs.

\begin{table}[htbp]
\centering
\caption{Relative energy efficiency of a single non-linear operation (ASNC vs.\ standard SiLU on GPU). Loihi~2 requires only $0.5\%$ of GPU energy at both 16-bit and 32-bit, a more than $200\times$ saving, while preserving accuracy within $2\%$ of the ANN baseline.}
\label{tab:hardware_efficiency}
\begin{tabular}{lcc}
\hline
\textbf{Precision} & \textbf{Time Steps} & \textbf{Energy Ratio (Loihi 2 SNN / GPU ANN)} \\
\hline
16-bit & 16 & $5.5 \times 10^{-3}$ \\
32-bit & 32 & $5.3 \times 10^{-3}$ \\
\hline
\end{tabular}
\end{table}
