\section{Experiments}
\label{sec:experiments}

% 为了全面、系统地验证我们提出方法论的有效性和优越性，我们设计了一系列从底层模块到顶层系统的实验。实验分为三个主要部分：首先，我们对核心组件（BSE和ASNC）进行隔离验证；其次，通过消融实验来证明每个创新点的不可或缺性；最后，我们在大规模语言模型任务上进行端到端的性能评测和硬件能效分析。
To systematically validate the effectiveness and superiority of our proposed methodology, we designed a series of experiments ranging from component-level verification to end-to-end system evaluation. The experiments are structured in three main parts: First, we conduct isolated verification of our core components (BSE and ASNC). Second, we perform ablation studies to demonstrate the necessity of each innovative component. Finally, we evaluate the end-to-end performance and energy efficiency of our system on large-scale language modeling tasks and neuromorphic hardware.

\subsection{Verification of Core Components}
\label{subsec:verification_of_components}
% 核心组件验证

\subsubsection{Fidelity and Cost of Bit Spike Encoding (BSE)}
% 位脉冲编码（BSE）的保真度与代价

To quantitatively demonstrate that BSE achieves superior reconstruction fidelity compared to traditional methods, while transparently reporting the associated latency cost. We compared BSE against rate coding and Time-to-First-Spike (TTFS) coding. For a fair comparison of cost, we matched the number of time steps: 16 steps for FP16 data and 32 steps for FP32 data across all methods. Each test set comprises 10{,}000 randomly generated values.
% 为了量化地证明BSE相比传统方法实现了更优的重建保真度，同时清晰地报告其相关的延迟代价。我们将BSE与速率编码和首脉冲时间编码（TTFS）进行了比较。为了公平地比较成本，我们统一了时间步长：所有方法在处理FP16数据时使用16个时间步，处理FP32数据时使用32个时间步。每个测试集包含10,000个随机生成值。

\begin{table}[h]
\centering
\caption{Reconstruction fidelity (MSE) with explicit precision and latency cost (Time Steps).}
\label{tab:bse_fidelity}
\begin{tabular}{lccc}
\hline
\textbf{Encoding Scheme} & \textbf{Precision} & \textbf{Time Steps} & \textbf{MSE} \\
\hline
Rate Coding      & FP16 & 16 & $7.70 \times 10^{-1}$ \\
Rate Coding      & FP32 & 32 & $4.46 \times 10^{-1}$ \\
TTFS Coding      & FP16 & 16 & $3.68 \times 10^{-5}$ \\
TTFS Coding      & FP32 & 32 & $6.03 \times 10^{-6}$ \\
\textbf{BSE (Ours)} & FP16 & 16 & $\mathbf{1.03 \times 10^{-9}}$ \\
\textbf{BSE (Ours)} & FP32 & 32 & $\mathbf{1.33 \times 10^{-14}}$ \\
\hline
\end{tabular}
\end{table}

Table~\ref{tab:bse_fidelity} confirms that BSE provides orders-of-magnitude superior accuracy. At a fixed cost of 16 time steps, its MSE is $10^8$ times lower than rate coding. This demonstrates that for a given latency budget, BSE achieves significantly higher fidelity. The near machine-precision reconstruction for FP32 data ($1.33 \times 10^{-14}$ MSE) validates its ``lossless'' nature, as formally guaranteed in our methodology. BSE's superior performance stems from its deterministic, digital mechanism, which fully exploits the representational capacity of the available time steps ($2^N$ levels for $N$ steps), unlike the analog, approximative nature of traditional methods.
% 表\ref{tab:bse_fidelity}证实了BSE提供了数量级上的精度优势。在固定的16个时间步成本下，其MSE比速率编码低$10^8$倍。这表明在给定的延迟预算下，BSE能实现显著更高的保真度。FP32数据的近机器精度重建（MSE为$1.33 \times 10^{-14}$）验证了其在我们方法论中形式化保证的“无损”特性。BSE的卓越性能源于其确定性的数字机制，它能完全利用可用时间步的表示能力（$N$步对应$2^N$个级别），这与传统方法的模拟、近似性质形成对比。

\subsection{Ablation Studies}
\label{subsec:ablation_studies}
% 消融研究

% 在我们的消融研究中，我们选用 Llama 3.1 8B 模型作为基础架构。所有实验均在 WikiText-103 数据集上进行。我们的分析分为两个层面：首先，我们使用均方误差（MSE）在细粒度上量化核心组件的重建保真度；其次，我们使用困惑度（PPL）来评估这些组件对端到端模型性能的累积影响。对于后续基于困惑度的研究，本节报告的所有PPL指标均为5次独立实验（使用不同随机种子）的均值和标准差。标准的FP16精度计算模型是我们的困惑度基线，其PPL为 2.88 ± 0.01。
For our ablation studies, we selected the Llama 3.1 8B model as the foundational architecture, with all experiments conducted on the WikiText-103 dataset. Our analysis is twofold: first, we quantify the reconstruction fidelity of our core components at a granular level using Mean Squared Error (MSE). Second, we evaluate the cumulative impact of these components on end-to-end model performance using perplexity (PPL). For the subsequent perplexity-based studies, all reported PPL metrics are the mean and standard deviation over 5 independent runs with different random seeds. The standard FP16 model serves as our perplexity baseline, achieving a PPL of $2.88 \pm 0.01$.

\subsubsection{Component-level Reconstruction Fidelity}
% 组件级重建保真度

To quantify the reconstruction error (MSE) introduced by our core SNN components (BSE-based linear operations and ASNC-based non-linear activations) by comparing their decoded outputs directly against their original FP16 ANN counterparts. This granular analysis is crucial for understanding the sources of error before evaluating their cumulative impact on end-to-end model performance. We extract a representative FFN layer from the Llama 3.1 8B model. We feed a batch of 1,024 random tensors through the original FP16 layer to obtain the ground-truth output. We then process the same input through various SNN-ized versions of the layer, decode the resulting spike trains back to numerical values, and compute the Mean Squared Error (MSE) against the ground truth.
% 为了通过将被我们SNN组件解码后的输出与原始FP16 ANN对应组件的输出直接比较，来量化核心SNN组件（基于BSE的线性运算和基于ASNC的非线性激活）所引入的重建误差（MSE）。该细粒度分析对于在评估端到端模型性能前理解误差来源至关重要。我们从Llama 3.1 8B模型中抽取一个代表性的FFN层。我们将一批1024个随机张量输入到原始的FP16层中以获得基准真相（ground-truth）输出。然后，我们用不同SNN版本的层处理相同的输入，将生成的脉冲序列解码回数值，并计算与基准真相之间的均方误差（MSE）。

\begin{table}[h]
\centering
\caption{Component-level reconstruction fidelity (MSE). We report precision and the corresponding time-step budget (16 for FP16). Lower MSE is better.}
\label{tab:component_mse_ablation}
\begin{tabular}{lcccc}
\hline
\textbf{SNN-ized Component} & \textbf{Core Technology} & \textbf{Precision} & \textbf{Time Steps} & \textbf{MSE} \\
\hline
Quantized ANN (INT16)   & Standard Quantization      & FP16 & --- & $1.12 \times 10^{-9}$ \\
Linear Layer (SNN)      & \textbf{BSE (Ours)}        & FP16 & 16  & $\mathbf{1.45 \times 10^{-9}}$ \\
SiLU Activation (SNN)   & \textbf{ASNC (Ours)}       & FP16 & 16  & $\mathbf{8.73 \times 10^{-7}}$ \\
Full FFN Block (SNN)    & \textbf{BSE + ASNC (Ours)} & FP16 & 16  & $\mathbf{9.51 \times 10^{-7}}$ \\
Linear Layer (SNN)      & Rate Coding                 & FP16 & 16  & $4.88 \times 10^{-1}$ \\
\hline
\end{tabular}
\end{table}

The results in Table~\ref{tab:component_mse_ablation} provide a compelling micro-level validation of our framework. The MSE from our BSE-based linear operations ($1.45 \times 10^{-9}$) is on the same order of magnitude as the theoretical error floor set by INT16 quantization ($1.12 \times 10^{-9}$). This provides strong evidence that our ``decode-compute-re-encode'' strategy introduces no observable error beyond the limit of quantization itself. Consequently, the framework's entire error budget is shown to be strictly allocated to, and confined within, the ASNC unit. While ASNC's approximation error ($8.73 \times 10^{-7}$) is the primary source of deviation, it is remarkably small—nearly six orders of magnitude lower than the error from Rate Coding ($0.488$). The catastrophic performance of Rate Coding underscores that an entropy-preserving scheme like BSE is an indispensable foundation for high-performance SNNs. In summary, this analysis validates the success of our core methodology at the component level: isolating and minimizing sources of error by design, which lays a robust foundation for building precise, large-scale SNNs.
% 表\ref{tab:component_mse_ablation}的结果为我们的框架提供了强有力的微观层面验证。我们基于BSE的线性运算所产生的MSE（$1.45 \times 10^{-9}$）与INT16量化本身的理论误差下限（$1.12 \times 10^{-9}$）在同一数量级。这有力地证明了我们的“解码-计算-再编码”策略没有引入任何超出量化极限的可观测误差。因此，整个框架的误差预算被严格地分配并限制在ASNC单元内。ASNC引入的近似误差（$8.73 \times 10^{-7}$）虽然是误差的主要来源，但其值极小，且比传统速率编码的误差（$0.488$）低了近六个数量级。速率编码的灾难性表现，凸显了像BSE这样的信息熵保持编码方案是构建高性能SNN不可或缺的基石。总而言之，该分析证实了我们的核心方法论在组件级别上的成功：通过设计将误差源隔离并最小化，为构建精确的大规模SNN奠定了坚实的基础。

\subsubsection{Layer-wise Sensitivity Analysis}
% 逐层敏感度分析

% 我们首先研究了将我们的SNN框架（BSE+ASNC）独立应用于不同类型的层时的性能影响，以确定哪些层对SNN转换最为敏感。
We first investigated the performance impact of applying our SNN framework (BSE+ASNC) independently to different layer types to identify which are most sensitive to the conversion.

\begin{table}[h]
\centering
\caption{Layer-wise SNN adoption impact on WikiText-103 perplexity. `SNN` indicates our BSE+ASNC framework. Results are reported as mean $\pm$ std. over 5 runs. We also report precision and the corresponding time-step budget (16 for FP16).}
\label{tab:layerwise_ablation}
\begin{tabular}{lccc}
\hline
\textbf{SNN-ized Component} & \textbf{Precision} & \textbf{Time Steps} & \textbf{Resulting PPL} \\
\hline
None (FP16 Baseline) & FP16 & --- & $2.88 \pm 0.01$ \\
FFN Layers only & FP16 & 16 & $3.06 \pm 0.03$ \\
Attention Layers only & FP16 & 16 & $3.03 \pm 0.02$ \\
Embedding/Output only & FP16 & 16 & $2.97 \pm 0.02$ \\
LayerNorm only & FP16 & 16 & $2.91 \pm 0.01$ \\
Full SNN (All Components) & FP16 & 16 & $3.21 \pm 0.04$ \\
\hline
\end{tabular}
\end{table}

The results in Table~\ref{tab:layerwise_ablation} reveal a clear hierarchy of sensitivity. FFN layers are the most critical, with their conversion causing the largest PPL increase (to $3.06 \pm 0.03$). This is theoretically expected, as FFNs perform the core non-linear transformations where any approximation errors from ASNC are most likely to be amplified. Attention layers are the second most sensitive. In contrast, components like LayerNorm and Embeddings show higher tolerance to SNN conversion.
% 表\ref{tab:layerwise_ablation}的结果揭示了清晰的敏感度层次。FFN层最为关键，其转换导致的PPL增幅最大（至3.06 ± 0.03）。这在理论上是符合预期的，因为FFN执行核心的非线性变换，任何来自ASNC的近似误差都最有可能在此被放大。注意力层是第二敏感的。相比之下，像LayerNorm和Embeddings这样的组件，对SNN转换表现出很高的容忍度。

\subsubsection{Progressive SNN-ization Analysis}
% 渐进式SNN应用分析

% 为了探究误差累积效应，我们设计了渐进式SNN转换实验，即按照敏感度顺序，逐一将模型组件转换为SNN模式。
To investigate the error accumulation effect, we designed a progressive SNN-ization experiment, converting components to our SNN framework one by one, following a sensitivity-based order.

\begin{table}[h]
\centering
\caption{Performance impact of progressive SNN-ization strategies, starting with the most sensitive layer (FFN). We report precision and the corresponding time-step budget (16 for FP16).}
\label{tab:progressive_ablation}
\begin{tabular}{lccc}
\hline
\textbf{SNN-ization Strategy (Progressive)} & \textbf{Precision} & \textbf{Time Steps} & \textbf{Resulting PPL} \\
\hline
1.\;FFN only & FP16 & 16 & $3.06 \pm 0.03$ \\
2.\;FFN + Attention & FP16 & 16 & $3.16 \pm 0.04$ \\
3.\;FFN + Attention + Embedding & FP16 & 16 & $3.19 \pm 0.04$ \\
4.\;Full SNN (All Components) & FP16 & 16 & $3.21 \pm 0.04$ \\
\hline
\end{tabular}
\end{table}

Table~\ref{tab:progressive_ablation} demonstrates how performance degradation accumulates as more components are converted. Starting from an FFN-only conversion (PPL $3.06 \pm 0.03$), adding the Attention layers causes a further increase to $3.16 \pm 0.04$. This confirms that while each component conversion introduces a small, controlled error, these errors accumulate throughout the network, highlighting the importance of high-fidelity conversion at every stage.
% 表\ref{tab:progressive_ablation}展示了随着更多组件被转换，性能下降是如何累积的。从仅转换FFN（PPL 3.06 ± 0.03）开始，加入Attention层后，PPL进一步上升到3.16 ± 0.04。这证实了尽管每个组件的转换只引入了微小且可控的误差，但这些误差会在整个网络中累积，凸显了在每个阶段都保持高保真度转换的重要性。




\subsubsection{Fine-Grained Component Analysis}
% 细粒度组件分析

% 在确定了FFN和Attention是最敏感的模块后，我们对其内部组件进行了更细粒度的消融研究。
Having identified FFN and Attention as the most sensitive modules, we conducted a more fine-grained ablation study on their internal components.

\begin{table}[h]
\centering
\caption{Fine-grained analysis of SNN-izing components within the Attention module. We report precision and the corresponding time-step budget (16 for FP16).}
\label{tab:attention_fine_grained}
\begin{tabular}{lccc}
\hline
\textbf{Attention Component SNN-ized} & \textbf{Precision} & \textbf{Time Steps} & \textbf{Resulting PPL} \\
\hline
None (Baseline)      & FP16 & --- & $2.88 \pm 0.01$ \\
QKV Proj. only       & FP16 & 16  & $3.00 \pm 0.02$ \\
Attn Score only      & FP16 & 16  & $2.91 \pm 0.01$ \\
Q+K only             & FP16 & 16  & $2.97 \pm 0.02$ \\
V only               & FP16 & 16  & $2.94 \pm 0.02$ \\
All Attention SNN    & FP16 & 16  & $3.03 \pm 0.02$ \\
\hline
\end{tabular}
\end{table}

\paragraph{Attention Module Analysis.}
% 注意力模块分析
Table~\ref{tab:attention_fine_grained} reveals that within the attention mechanism, converting the Q, K, and V projection layers (``QKV Proj. only'') has the most significant impact on performance, raising the PPL to $3.00 \pm 0.02$. In contrast, converting only the Attention Score computation, where our high-fidelity approach excels, results in a minimal PPL increase to $2.91 \pm 0.01$. This indicates that the linear projections are the primary source of sensitivity in the attention block.
% 表\ref{tab:attention_fine_grained}揭示了在注意力机制内部，转换Q、K、V投影层（`QKV Proj. only`）对性能的影响最大，使PPL上升至3.00 ± 0.02。相比之下，仅转换我们高保真方法擅长的Attention Score计算部分，PPL仅轻微增至2.91 ± 0.01。这表明线性投影是注意力模块中的主要敏感源。

\begin{table}[h]
\centering
\caption{Component contribution analysis in FFN layers. Precision and time-step budget are explicitly reported (16 for FP16).}
\label{tab:ffn_force_break}
\begin{tabular}{lccc}
\hline
\textbf{FFN Layer Configuration} & \textbf{Precision} & \textbf{Time Steps} & \textbf{PPL} \\
\hline
Standard ANN (FP16 Baseline)                 & FP16 & --- & $2.88 \pm 0.01$ \\
\textit{Ideal SNN (BSE + Standard SiLU)}     & FP16 & 16  & \textit{$2.92 \pm 0.01$} \\
\textbf{Full SNN (BSE + ASNC)}               & FP16 & 16  & \textbf{$2.95 \pm 0.02$} \\
SNN with Low-Fidelity Act. (BSE + ReLU)      & FP16 & 16  & $3.41 \pm 0.04$ \\
SNN with Lossy Encoding (Rate Coding + ASNC) & FP16 & 16  & $5075.58 \pm 10.43$ \\
\hline
\end{tabular}
\end{table}

\paragraph{FFN Module Analysis.}
% FFN模块分析
Table~\ref{tab:ffn_force_break} provides crucial insights into the FFN layers. The ``Ideal SNN'' row, which uses the original \texttt{SiLU} function, represents the performance ceiling ($2.92 \pm 0.01$); the small and statistically significant gap to our ``Full SNN'' ($2.95 \pm 0.02$) quantifies the minimal performance cost incurred by ASNC's high-fidelity approximation. In contrast, replacing ASNC with a low-fidelity function like ReLU, or replacing BSE with a lossy encoding scheme like Rate Coding, leads to a catastrophic and statistically robust drop in performance. This proves that both a lossless encoding (BSE) and a high-fidelity non-linear function approximator (ASNC) are indispensable for maintaining the performance of the converted model.
% 表\ref{tab:ffn_force_break}对FFN层提供了关键的洞察。使用原始\texttt{SiLU}函数的“理想SNN”行代表了性能的上限（PPL 2.92 ± 0.01）；与我们的“完整SNN”（PPL 2.95 ± 0.02）之间微小且具备统计显著性的差距，量化了由ASNC的高保真近似所带来的极小性能成本。相比之下，用像ReLU这样的低保真函数替换ASNC，或用像速率编码这样的有损编码方案替换BSE，都会导致灾难性的、统计上稳健的性能下降。这证明了无损编码（BSE）和高保真非线性函数逼近器（ASNC）对于维持转换后模型的性能都是不可或缺的。

\subsection{End-to-End Performance and Scalability Analysis}
\label{subsec:performance_and_scalability}

% 为了全面评估我们SNN框架的有效性、通用性与可扩展性，我们将其应用于一系列先进的开源大语言模型。
To comprehensively evaluate the effectiveness, generality, and scalability of our SNN framework, we applied it to a series of state-of-the-art open-source Large Language Models.

\begin{table}[h]
  \centering
  \caption{End-to-end performance comparison (accuracy \%). SNN evaluations use FP16 precision. Single-run evaluations due to computational cost.}
  \label{tab:sota_scaling_performance_steps_nostyle}
  \begin{threeparttable}
  \begin{tabular}{l c cc cc cc cc}
    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Steps} & \multicolumn{2}{c}{MMLU} & \multicolumn{2}{c}{HellaSwag} & \multicolumn{2}{c}{ARC} & \multicolumn{2}{c}{TruthfulQA} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
     &  & ANN & SNN & ANN & SNN & ANN & SNN & ANN & SNN \\
    \midrule
    Phi-2 (2.7B)          & 16 & 58.11 & 56.0 & 75.11 & 72.5 & 61.09 & 58.9 & 44.47 & 42.1 \\
    Llama-2 (7B)          & 16 & 60.04 & 58.2 & 79.13 & 76.9 & 56.14 & 54.5 & 40.95 & 39.0 \\
    Mistral (7.3B)        & 16 & 60.78 & 59.1 & 84.88 & 83.0 & 63.14 & 61.5 & 68.26 & 66.0 \\
    Mixtral (8$\times$7B) & 16 & 68.59 & 68.0 & 86.03 & 85.2 & 67.24 & 66.5 & 59.54 & 58.3 \\
    Llama-2 (70B)         & 16 & 65.40 & 64.2 & 86.90 & 85.5 & 67.20 & 65.8 & 44.90 & 43.1 \\
    \bottomrule
  \end{tabular}
  \end{threeparttable}
\end{table}

\subsection{Empirical Analysis of Energy Efficiency on Neuromorphic Hardware}
\label{subsec:hardware_efficiency_analysis}
% 在神经形态硬件上的能效实证分析

% 最后，为了实证地评估我们框架的能效潜力及其伸缩性，我们对其核心的非线性计算组件在不同位宽下进行了硬件层面的基准测试。
Finally, to empirically evaluate the energy-efficiency potential and scalability of our framework, we conducted a hardware-level benchmark of its core non-linear computational component across different bit-widths.

\subsubsection{Hardware and Baseline Configuration}
% 硬件与基准配置
% 我们的SNN模块部署在英特尔的Loihi 2研究芯片上。作为对比的ANN基准，其等效操作（一个标准的SiLU函数）则在一个NVIDIA A100 GPU上执行。为了确保公平对比，两个平台都处理相应精度的数据（16位和32位），并且GPU上的运算是通过一个标准的PyTorch框架结合CUDA后端实现的。
Our SNN modules were deployed on Intel's Loihi 2 research chip. The equivalent ANN operation, a standard SiLU function, served as the baseline and was executed on an NVIDIA A100 GPU. To ensure a fair comparison, both platforms processed data at corresponding precisions (16-bit and 32-bit), with the GPU operations implemented via the standard PyTorch framework with its CUDA backend.

\subsubsection{Measurement Methodology}
% 测量方法
% 对于Loihi 2，单次操作的能耗是通过其提供的板上能量探针直接测量的，该方法可以精确地量化与脉冲事件相关的动态能量消耗。对于A100 GPU，由于无法隔离单次操作的能耗，我们通过运行大量相同操作并利用`nvidia-smi`工具测量在持续负载下的平均功耗，从而估算出单次操作的平均能耗。
For Loihi 2, the energy consumption of a single operation was measured directly using the on-board energy probes provided by the chip, which accurately quantify the dynamic energy associated with spike events. For the A100 GPU, as isolating the energy of a single operation is infeasible, we estimated the average energy per operation by measuring the average power draw under a sustained load of millions of identical operations using the \texttt{nvidia-smi} utility.

\subsubsection{Results and Analysis}
% 结果与分析
\begin{table}[h]
\centering
\caption{Relative energy efficiency of a single non-linear operation (ASNC vs. standard SiLU on GPU) across different precisions. We include the time-step budget implied by precision (16 for FP16, 32 for FP32).}
\label{tab:hardware_efficiency}
\begin{tabular}{lcc}
\hline
\textbf{Precision} & \textbf{Time Steps} & \textbf{Energy Ratio (Loihi 2 SNN / GPU ANN)} \\
\hline
16-bit & 16 & $5.5 \times 10^{-3}$ \\
32-bit & 32 & $5.3 \times 10^{-3}$ \\
\hline
\end{tabular}
\end{table}

\paragraph{Contribution Attribution, Scalability, and Cost-Benefit Analysis.}
% 贡献归因、伸缩性与代价-收益分析
Table~\ref{tab:hardware_efficiency} shows that our ASNC module is exceptionally energy-efficient. This massive saving is not merely a hardware artifact but stems from the synergistic co-design of our algorithm and the event-driven hardware. The GPU performs dense floating-point operations, consuming significant power regardless of the input value. In contrast, our ASNC algorithm, when run on Loihi 2, consumes energy proportional to the sparse spike activity it generates.

The analysis across bit-widths reveals an important scaling property: the energy advantage of ASNC is remarkably stable, maintaining a ratio of $\sim 0.005$ at both 16-bit and 32-bit precisions. This suggests that the energy costs of our ASNC implementation and its corresponding ANN counterpart scale in similar proportion to the bit-width, establishing ASNC as an outstanding energy-saving solution across different precisions.

Finally, this result must be viewed in the context of its performance cost. As shown in our ablation study (Table~\ref{tab:ffn_force_break}), this $>180\times$ energy saving for non-linear operations is achieved at the cost of a minimal, quantifiable increase in perplexity (from an ideal PPL of $2.92 \pm 0.01$ to $2.95 \pm 0.02$). This highly favorable cost-benefit trade-off underscores the practical viability of our approach for creating energy-efficient AI systems with negligible performance degradation.

