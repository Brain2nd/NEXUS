\section{Related Works}
\label{sec:related_works}

High-fidelity mapping and trainability from ANNs to SNNs have long been constrained by three fundamental obstacles: first, the equivalence between numerical activations and spike sequence representations is difficult to establish, and simple replacement leads to decreased information density and soaring latency; second, the spiking implementations of nonlinear numerical computations such as ReLU/Softmax/BN lack structural generalization capability, with errors often diffusing at multiple points within the network; third, temporal dependency and binary firing cause backpropagation to suffer from issues such as unstable training, diffusion of information loss, and uncontrollable errors.

In the direct conversion from numerical to spiking representations, the traditional paths exhibit clear limitations. Conventional rate coding, although easy to use, has low information density, poor expressive efficiency, and high latency; for example, to approximate 0.625, it often requires accumulating spike counts over a long time window to form a reliable estimate. Such “high-latency approximations” have long been pointed out in surveys to amplify jitter and estimation variance and slow convergence. Mainstream ANN→SNN conversion works, while systematizing equivalent implementations of operators such as threshold/weight normalization, Softmax, and BN, are still essentially dominated by rate/time-window estimation (and thus limited in latency and energy efficiency). Time-to-first-spike (TTFS) or first-spike coding can improve speed within short time windows, but accuracy and robustness are often constrained by the single-spike requirement and training stability. To establish a stable, equivalent, and tunable spiking representation, we introduce BSE (Bit Spike Encoding), which establishes a one-to-one bijection between a deterministic N-step spike sequence and any N-bit numerical value, achieving entropy preservation and reversible decoding under ideal discrete, noiseless, and lossless conditions. This fundamentally avoids the many-to-one information loss and error diffusion of traditional approximate encodings, while its cost is that latency is linearly proportional to bit width, forming a fixed and predictable delay budget (e.g., FP16 requires 16 time steps).

Constructing nonlinear transformations in the spiking domain is highly challenging, and nonlinear operators provide networks with representational power and separability, so they cannot be skipped. In our setting, since the introduction of BSE makes the inputs/outputs of each layer spike sequences carrying temporal dimensions, the original ANN nonlinearities (such as SiLU/Softmax) can no longer be directly used, and precise and controllable nonlinear implementations must be given in the spiking domain. Existing paths either rely on delay/time-window “simulation-style” approximations, or tailor “spike-friendly” structures for local components, such as the Spikformer series using Spiking Self-Attention (SSA) to realize Q/K/V in spiking form and avoid softmax, or through structural rewrites to circumvent non-spiking operators in attention/residual modules. Other works provide direct spiking equivalents of softmax/LayerNorm at the conversion level to support Transformer conversion, but overall remain limited by approximation errors and generalization, and cannot constrain errors strictly within a single nonlinear component. Based on this, we propose a pluggable structural approximator, ASNC, to provide high-fidelity and controllable approximation, and to strictly confine errors within the nonlinear unit, avoiding their diffusion through the network.

In terms of training, SNNs are usually constrained by the problems of non-differentiable gradients and discontinuous information. Although frameworks such as STBP, SLAYER, DIET-SNN, and hybrid conversion + fine-tuning have been developed, many approaches require special structures, long time windows, or multi-stage processes, which in practice introduce training instability and uncontrollable errors. Given that our designed forward path already possesses high consistency, we can introduce lightweight STE substitutions on this basis for stability optimization. This aligns with the theoretical and empirical consensus of surrogate gradients/STE and is consistent in engineering goals with low-timestep-oriented training methods such as TET.

In recent years, a series of explorations of large SNN models have achieved initial progress on Transformer/LLM tasks and shown certain potential, but these methods mostly rely on long time windows/high firing rates or surrogate gradients, approximately differentiable activations, and other “soft substitutes” to obtain differentiability. They do not confine errors strictly to single points (such as nonlinear units), and errors easily diffuse along temporal and depth dimensions, causing unstable training. Thus, under short-timestep budgets and ultra-large-scale modeling, it is difficult to simultaneously maintain strong structural consistency with the original ANN and preserve high-fidelity accuracy, limiting engineering deployment. For example: Spikformer/Spikformer-v2 replace standard attention with SSA or softmax-free attention; SpikeGPT/Spiking-LLM use soft gating, approximately differentiable activations, and gated spiking units for autoregression; Spike-Driven Transformer and Spiked-Attention/Spiking Self-Attention perform spike-friendly structural rewrites in Q/K/V and normalization paths; SpikeZIP/SpikeZIP-TF align with ANNs through long time windows and reparameterization, requiring spike-friendly activations for training alignment; SpikeLM explores direct training from scratch. To address these challenges, we propose a roadmap for gradually approaching an ideal SNN, with the core objective of maximizing the preservation of information expression capability, nonlinear modeling capability, and tunability, while controlling errors within predictable single-point regions of the network structure.
